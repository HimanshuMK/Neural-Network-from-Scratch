{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOc5NVKnecEL/E+OBkVc3Xa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimanshuMK/Neural-Network-from-Scratch/blob/main/FullCodeNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "3_Q9uoqAJhht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5CIkZFqJHum0"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Dense Layer\n",
        "The Layer_Dense class implements a dense (fully connected) layer in a neural network.\n",
        "\n",
        "1. Initialization (`__init__` method):\n",
        "  *   Takes the number of inputs (n_inputs) and the number of neurons (n_neurons).\n",
        "  *   Initializes the weights with small random values and biases with zeros.\n",
        "\n",
        "\n",
        "2. Forward Pass (`forward` method):\n",
        "  *   Takes the input data (inputs).\n",
        "  *   Computes the output by performing a dot product of inputs and weights, then adds biases.\n",
        "  *   Stores the input values for use in the backward pass.\n",
        "\n",
        "\n",
        "3. Backward Pass (`backward` method):\n",
        "  * Takes the gradient of the loss with respect to the output (dvalues).\n",
        "  * Computes the gradients with respect to weights (dweights), biases (dbiases), and inputs (dinputs).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "etZn9pJ5JnfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "metadata": {
        "id": "6H_wByjvHyt6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean Squared Error Loss and its gradient\n",
        "\n",
        "1.   Mean Squared Error Loss (`mse_loss` function):\n",
        "    *   Takes the predicted values (y_pred) and the true values (y_true).\n",
        "    *   Calculates the Mean Squared Error by computing the average of the squared differences between y_pred and y_true.\n",
        "\n",
        "\n",
        "2.   Gradient of Mean Squared Error Loss (`mse_loss_gradient` function):\n",
        "    *   Takes the predicted values (y_pred) and the true values (y_true).\n",
        "    *   Computes the gradient of the Mean Squared Error with respect to the predictions. This is calculated as 2 * (y_pred - y_true) / y_true.size, which gives the rate of change of the loss with respect to the predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "pcU1bqRBvCta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error Loss and its gradient\n",
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def mse_loss_gradient(y_pred, y_true):\n",
        "    return 2 * (y_pred - y_true) / y_true.size"
      ],
      "metadata": {
        "id": "ZmnU3P6wHyw9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReLU activation\n",
        "\n",
        "1.   Forward Pass (`forward` method):\n",
        "    * Takes the input data (inputs).\n",
        "    * Computes the output by applying the ReLU function, which sets all negative values to zero and keeps positive values unchanged.\n",
        "    * Stores the input values for use in the backward pass.\n",
        "2.   Backward Pass (`backward` method):\n",
        "    * Takes the gradient of the loss with respect to the output (dvalues).\n",
        "    * Copies dvalues to self.dinputs.\n",
        "    * Sets the gradient to zero where the original input values were negative (because the derivative of ReLU is zero for negative inputs).\n",
        "\n"
      ],
      "metadata": {
        "id": "PyCd8_NGvhEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum( 0 , inputs)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Since we need to modify original variable,\n",
        "    # we make a copy of values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero gradient where input values were negative\n",
        "    self.dinputs[self.inputs <= 0 ] = 0"
      ],
      "metadata": {
        "id": "MPtnJjloHy0b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Softmax Activation\n",
        "\n",
        "1.   Forward Pass (`forward` method):\n",
        "    * Takes the input data (inputs).\n",
        "    * Computes the exponentiated values of the inputs after subtracting the maximum value in each input for numerical stability.\n",
        "    * Normalizes these exponentiated values to calculate the probabilities, ensuring that they sum to 1.\n",
        "    * Stores the output probabilities.\n",
        "2.   Backward Pass (`backward` method):\n",
        "    * Takes the gradient of the loss with respect to the output (dvalues).\n",
        "    * Initializes an array for the gradients with the same shape as dvalues.\n",
        "    * For each sample in the batch:\n",
        "        * Reshapes the output to a column vector.\n",
        "        * Calculates the Jacobian matrix of the softmax function for the sample.\n",
        "        * Computes the gradient of the loss with respect to the input using the Jacobian matrix and the gradient of the loss with respect to the output.\n"
      ],
      "metadata": {
        "id": "5YqbxZHNwMFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Softmax Activation\n",
        "class Activation_Softmax:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        # Subtract max value for numerical stability\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        # Normalizing probabilities\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Creating uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "        # Enumerating through outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "            # Flattening output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output and its gradient\n",
        "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n"
      ],
      "metadata": {
        "id": "MPYS794pIK2D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Cross-Entropy Loss and Gradient\n",
        "\n",
        "1.   Categorical Cross-Entropy Loss (`categorical_crossentropy_loss` function):\n",
        "    * Takes the predicted probabilities (y_pred) and the true labels (y_true).\n",
        "    * Clips the predicted probabilities to avoid division by zero.\n",
        "    * Calculates the loss by taking the negative logarithm of the predicted probabilities corresponding to the true labels.\n",
        "    * Returns the average loss over all samples.\n",
        "\n",
        "2.   Gradient of Categorical Cross-Entropy Loss(`categorical_crossentropy_loss_gradient` function):\n",
        "    * Takes the predicted probabilities (y_pred) and the true labels (y_true).\n",
        "    * Clips the predicted probabilities to avoid division by zero.\n",
        "    * Converts true labels to one-hot encoding if they are not already.\n",
        "    * Computes the gradient of the loss with respect to the predicted probabilities by taking the negative ratio of the true labels to the predicted probabilities.\n",
        "    * Normalizes the gradients by the number of samples.\n",
        "    * Returns the gradients.\n",
        "\n"
      ],
      "metadata": {
        "id": "lL90zx0ow-YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Softmax Activation\n",
        "# Categorical Cross-Entropy Loss\n",
        "def categorical_crossentropy_loss(y_pred, y_true):\n",
        "    # Clip data to prevent division by 0\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    # Calculate loss\n",
        "    correct_confidences = y_pred[range(len(y_pred)), y_true]\n",
        "    return -np.mean(np.log(correct_confidences))\n",
        "\n",
        "\n",
        "# Gradient of Categorical Cross-Entropy Loss\n",
        "def categorical_crossentropy_loss_gradient(y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    labels = len(y_pred[0])\n",
        "\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Initialize the gradient\n",
        "    gradients = np.zeros_like(y_pred)\n",
        "\n",
        "    # Convert labels to one-hot if they are not\n",
        "    if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]  #eye matrix\n",
        "\n",
        "    # Calculate gradient\n",
        "    gradients = -y_true / y_pred\n",
        "    gradients = gradients / samples\n",
        "\n",
        "    return gradients\n"
      ],
      "metadata": {
        "id": "uDcb1pRSIK4r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Common SGD Vanilla optimizer\n",
        "1.   Initialization (`__init__` method):\n",
        "    * Sets the initial learning rate (learning_rate) and decay rate (decay).\n",
        "    * Initializes current_learning_rate with the initial learning rate.\n",
        "    * Initializes the iteration counter (iterations) to zero.\n",
        "2.   Pre-Update Parameters (`pre_update_params` method):\n",
        "    * Adjusts the current learning rate based on the decay rate and the number of iterations.\n",
        "    * The formula used is:\n",
        "  $$current\\_learning\\_rate = learning\\_rate * \\frac{1}{1 + decay * iterations}$$\n",
        "3.   Update Parameters (`update_params` method):\n",
        "    * Updates the weights and biases of a layer by subtracting the product of the current learning rate and the gradients (dweights and dbiases).\n",
        "4.   Post-Update Parameters (`post_update_params` method):\n",
        "    * Increments the iteration counter by one after the parameters have been updated.\n",
        "\n"
      ],
      "metadata": {
        "id": "_G2NmxZ1yGsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## optimizers\n",
        "#Common SGD Vanilla optimizer\n",
        "class Optimizer_SGD :\n",
        "\n",
        "  # Initialize optimizer - set settings,\n",
        "  # learning rate of 1 is default for this optimizer\n",
        "  def __init__ ( self , learning_rate = 1.0, decay = 0.):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate *( 1. / ( 1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "\n",
        "    layer.weights += - self.current_learning_rate * layer.dweights\n",
        "    layer.biases += - self.current_learning_rate * layer.dbiases\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "UoZa6zFQIK7c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD optimizer with momentum and vanilla\n",
        "\n",
        "1. Initialization (`__init__` method):\n",
        "  * Sets the initial learning rate (learning_rate), decay rate (decay), and momentum factor (momentum).\n",
        "  * Initializes current_learning_rate with the initial learning rate.\n",
        "  * Initializes the iteration counter (iterations) to zero.\n",
        "2. Pre-Update Parameters (`pre_update_params` method):\n",
        "  * Adjusts the current learning rate based on the decay rate and the number of iterations.\n",
        "  * The formula used is:\n",
        "  $$ current\\_learning\\_rate = learning\\_rate * \\frac{1} {(1 + decay * iterations)}.$$\n",
        "3. Update Parameters (`update_params` method):\n",
        "  * With Momentum:\n",
        "    * Checks if the layer has momentum arrays for weights and biases. If not, initializes them to zero.\n",
        "    * Computes weight and bias updates using the momentum term and the current gradients.\n",
        "    * Updates the layer's momentum arrays with these new values.\n",
        "  * Without Momentum:\n",
        "    * Computes weight and bias updates using only the current gradients.\n",
        "  * Updates the weights and biases of the layer using the computed updates.\n",
        "4. Post-Update Parameters (`post_update_params` method):\n",
        "  * Increments the iteration counter by one after the parameters have been updated.\n"
      ],
      "metadata": {
        "id": "mCRC8QHpy44g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer with momentum and vanilla\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "nO12429tIK-M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam optimizer\n",
        "1. Initialization (`__init__` method):\n",
        "  * Sets initial values for learning rate (`learning_rate`), decay rate (`decay`), epsilon (`epsilon`), and exponential decay rates for the first (`beta_1`) and second (`beta_2`) moment estimates.\n",
        "  * Initializes current_learning_rate with the initial learning rate and the iteration counter (`iterations`) to zero.\n",
        "2. Pre-Update Parameters (`pre_update_params` method):\n",
        "  * Adjusts the current learning rate based on the decay rate and the number of iterations using the formula:\n",
        "  $$current\\_learning\\_rate = learning\\_rate * \\frac {1} {1 + decay * iterations}$$\n",
        "3. Update Parameters (`update_params` method):\n",
        "  * Initialization:\n",
        "    * Checks if the layer has momentum and cache arrays for weights and biases. If not, initializes them to zero.\n",
        "  * Momentum Update:\n",
        "    * Updates the momentum terms for weights and biases using the current gradients and the beta_1 parameter.\n",
        "  * Bias-Corrected Momentum:\n",
        "    * Computes bias-corrected momentum terms for weights and biases to counteract the initialization bias.\n",
        "  * Cache Update:\n",
        "    * Updates the cache terms for weights and biases using the squared current gradients and the beta_2 parameter.\n",
        "  * Bias-Corrected Cache:\n",
        "    * Computes bias-corrected cache terms for weights and biases to counteract the initialization bias.\n",
        "  * Parameter Update:\n",
        "    * Updates the weights and biases using the corrected momentum and cache terms, normalizing with the square root of the cache terms and adding epsilon for numerical stability.\n",
        "4. Post-Update Parameters (`post_update_params` method):\n",
        "  * Increments the iteration counter by one after the parameters have been updated."
      ],
      "metadata": {
        "id": "-hnELRY30F7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "8kirxVRLILBA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Layer with regularization\n",
        "\n",
        "The Layer_Dense class implements a dense (fully connected) layer in a neural network, including support for L1 and L2 regularization.\n",
        "1. Initialization (__init__ method):\n",
        "  * Takes the number of inputs (n_inputs), number of neurons (n_neurons), and regularization strengths for weights and biases (L1 and L2).\n",
        "  * Initializes weights with small random values and biases with zeros.\n",
        "  * Sets the regularization strengths for weights and biases.\n",
        "2. Forward Pass (forward method):\n",
        "  * Takes the input data (inputs).\n",
        "  * Computes the output by performing a dot product of inputs and weights, then adds biases.\n",
        "  * Stores the input values for use in the backward pass.\n",
        "3. Backward Pass (backward method):\n",
        "  * Takes the gradient of the loss with respect to the output (dvalues).\n",
        "  * Computes the gradients with respect to weights (dweights) and biases (dbiases).\n",
        "  * Adds the regularization gradients to dweights and dbiases:\n",
        "    * L1 Regularization: Adds the sign of the weights or biases multiplied by the L1 regularization strength.\n",
        "    * L2 Regularization: Adds twice the weights or biases multiplied by the L2 regularization strength.\n",
        "  * Computes the gradient of the loss with respect to the input values (dinputs).\n"
      ],
      "metadata": {
        "id": "Ay3J9jIGlPEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
      ],
      "metadata": {
        "id": "QL2oHN5V8z3q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization Loss\n",
        "The regularization_loss function calculates the total regularization loss for a given layer, incorporating both L1 and L2 regularization for weights and biases.\n",
        "1. Initialization:\n",
        "  * Initializes regularization_loss to 0.\n",
        "2. L1 Regularization on Weights:\n",
        "  * Adds the L1 regularization loss for weights if weight_regularizer_l1 is greater than 0.\n",
        "  * The L1 loss is calculated as the product of weight_regularizer_l1 and the sum of the absolute values of the weights.\n",
        "3. L2 Regularization on Weights:\n",
        "  * Adds the L2 regularization loss for weights if weight_regularizer_l2 is greater than 0.\n",
        "  * The L2 loss is calculated as the product of weight_regularizer_l2 and the sum of the squared weights.\n",
        "4. L1 Regularization on Biases:\n",
        "  * Adds the L1 regularization loss for biases if bias_regularizer_l1 is greater than 0.\n",
        "  * The L1 loss is calculated as the product of bias_regularizer_l1 and the sum of the absolute values of the biases.\n",
        "5. L2 Regularization on Biases:\n",
        "  * Adds the L2 regularization loss for biases if bias_regularizer_l2 is greater than 0.\n",
        "  * The L2 loss is calculated as the product of bias_regularizer_l2 and the sum of the squared biases.\n",
        "6. Return Total Regularization Loss:\n",
        "  * Returns the total regularization loss computed from the above steps."
      ],
      "metadata": {
        "id": "pf9VO-F_mYDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization loss calculation\n",
        "def regularization_loss(layer):\n",
        "    # 0 by default\n",
        "    regularization_loss = 0\n",
        "    # L1 regularization - weights\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.weight_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
        "    # L2 regularization - weights\n",
        "    if layer.weight_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
        "\n",
        "    # L1 regularization - biases\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.bias_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
        "    # L2 regularization - biases\n",
        "    if layer.bias_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
        "    return regularization_loss"
      ],
      "metadata": {
        "id": "XSRjMejmzEAB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jUQQKYqozDnb"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}