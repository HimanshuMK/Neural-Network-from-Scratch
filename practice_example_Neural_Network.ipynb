{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5RjAulaTDjgKro5P/aPl9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimanshuMK/Neural-Network-from-Scratch/blob/main/practice_example_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oqw8nDcLdl2c"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Layer"
      ],
      "metadata": {
        "id": "KXHbvKMxo6oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "metadata": {
        "id": "Z5n_EWk5dmQI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error Loss and its gradient\n",
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def mse_loss_gradient(y_pred, y_true):\n",
        "    return 2 * (y_pred - y_true) / y_true.size"
      ],
      "metadata": {
        "id": "inBwTDLFeBJu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Neural Network for 3 input features and 1 sample and trying to fit it in the model with output as 1 neuron"
      ],
      "metadata": {
        "id": "ZVNTd9rspEtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)  # For reproducibility\n",
        "layer = Layer_Dense(n_inputs=3, n_neurons=1)\n",
        "print(\"Initial weights:\", layer.weights)\n",
        "print(\"Initial biases:\", layer.biases)\n",
        "\n",
        "inputs = np.array([[1.0, 2.0, 3.0]])\n",
        "y_true = np.array([[1.0]])\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Training loop\n",
        "for i in range(20):  # Perform 5 iterations\n",
        "    # Forward pass\n",
        "    layer.forward(inputs)\n",
        "    y_pred = layer.output\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = mse_loss(y_pred, y_true)\n",
        "\n",
        "    # Compute the gradient of the loss with respect to the output\n",
        "    dvalues = mse_loss_gradient(y_pred, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    layer.backward(dvalues)\n",
        "\n",
        "    # Update weights and biases\n",
        "    layer.weights -= learning_rate * layer.dweights\n",
        "    layer.biases -= learning_rate * layer.dbiases\n",
        "\n",
        "    # Print the progress\n",
        "    print(f\"Iteration {i+1}:\")\n",
        "    print(\"Inputs:\", layer.inputs)\n",
        "    print(\"True output:\", y_true)\n",
        "    print(\"Predicted output:\", y_pred)\n",
        "    print(\"Loss:\", loss)\n",
        "    print(\"Updated weights:\", layer.weights)\n",
        "    print(\"Updated biases:\", layer.biases)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-GYJ5ELeN4B",
        "outputId": "c8db28d6-951c-42b9-83b6-a3e985b884d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights: [[ 0.04967142]\n",
            " [-0.01382643]\n",
            " [ 0.06476885]]\n",
            "Initial biases: [[0.]]\n",
            "Iteration 1:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.21632512]]\n",
            "Loss: 0.6141463230332932\n",
            "Updated weights: [[0.06534491]\n",
            " [0.01752057]\n",
            " [0.11178935]]\n",
            "Updated biases: [[0.0156735]]\n",
            "\n",
            "Iteration 2:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.45142758]]\n",
            "Loss: 0.3009316982863137\n",
            "Updated weights: [[0.07631636]\n",
            " [0.03946346]\n",
            " [0.14470369]]\n",
            "Updated biases: [[0.02664495]]\n",
            "\n",
            "Iteration 3:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.61599931]]\n",
            "Loss: 0.1474565321602936\n",
            "Updated weights: [[0.08399638]\n",
            " [0.05482349]\n",
            " [0.16774373]]\n",
            "Updated biases: [[0.03432496]]\n",
            "\n",
            "Iteration 4:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.73119951]]\n",
            "Loss: 0.07225370075854394\n",
            "Updated weights: [[0.08937238]\n",
            " [0.06557551]\n",
            " [0.18387176]]\n",
            "Updated biases: [[0.03970097]]\n",
            "\n",
            "Iteration 5:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.81183966]]\n",
            "Loss: 0.035404313371686535\n",
            "Updated weights: [[0.09313559]\n",
            " [0.07310192]\n",
            " [0.19516138]]\n",
            "Updated biases: [[0.04346418]]\n",
            "\n",
            "Iteration 6:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.86828776]]\n",
            "Loss: 0.01734811355212638\n",
            "Updated weights: [[0.09576984]\n",
            " [0.07837041]\n",
            " [0.20306412]]\n",
            "Updated biases: [[0.04609842]]\n",
            "\n",
            "Iteration 7:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.90780143]]\n",
            "Loss: 0.008500575640541916\n",
            "Updated weights: [[0.09761381]\n",
            " [0.08205835]\n",
            " [0.20859603]]\n",
            "Updated biases: [[0.04794239]]\n",
            "\n",
            "Iteration 8:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.935461]]\n",
            "Loss: 0.00416528206386555\n",
            "Updated weights: [[0.09890459]\n",
            " [0.08463991]\n",
            " [0.21246837]]\n",
            "Updated biases: [[0.04923317]]\n",
            "\n",
            "Iteration 9:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.9548227]]\n",
            "Loss: 0.0020409882112941186\n",
            "Updated weights: [[0.09980813]\n",
            " [0.08644701]\n",
            " [0.21517901]]\n",
            "Updated biases: [[0.05013672]]\n",
            "\n",
            "Iteration 10:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.96837589]]\n",
            "Loss: 0.0010000842235341153\n",
            "Updated weights: [[0.10044062]\n",
            " [0.08771197]\n",
            " [0.21707646]]\n",
            "Updated biases: [[0.0507692]]\n",
            "\n",
            "Iteration 11:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.97786312]]\n",
            "Loss: 0.0004900412695317219\n",
            "Updated weights: [[0.10088335]\n",
            " [0.08859745]\n",
            " [0.21840467]]\n",
            "Updated biases: [[0.05121194]]\n",
            "\n",
            "Iteration 12:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.98450419]]\n",
            "Loss: 0.00024012022207054202\n",
            "Updated weights: [[0.10119327]\n",
            " [0.08921728]\n",
            " [0.21933442]]\n",
            "Updated biases: [[0.05152185]]\n",
            "\n",
            "Iteration 13:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.98915293]]\n",
            "Loss: 0.00011765890881456583\n",
            "Updated weights: [[0.10141021]\n",
            " [0.08965116]\n",
            " [0.21998524]]\n",
            "Updated biases: [[0.0517388]]\n",
            "\n",
            "Iteration 14:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.99240705]]\n",
            "Loss: 5.7652865319137257e-05\n",
            "Updated weights: [[0.10156207]\n",
            " [0.08995488]\n",
            " [0.22044082]]\n",
            "Updated biases: [[0.05189065]]\n",
            "\n",
            "Iteration 15:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.99468494]]\n",
            "Loss: 2.8249904006376786e-05\n",
            "Updated weights: [[0.10166837]\n",
            " [0.09016748]\n",
            " [0.22075972]]\n",
            "Updated biases: [[0.05199696]]\n",
            "\n",
            "Iteration 16:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.99627946]]\n",
            "Loss: 1.3842452963124873e-05\n",
            "Updated weights: [[0.10174278]\n",
            " [0.0903163 ]\n",
            " [0.22098295]]\n",
            "Updated biases: [[0.05207137]]\n",
            "\n",
            "Iteration 17:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.99739562]]\n",
            "Loss: 6.782801951931129e-06\n",
            "Updated weights: [[0.10179487]\n",
            " [0.09042048]\n",
            " [0.22113922]]\n",
            "Updated biases: [[0.05212345]]\n",
            "\n",
            "Iteration 18:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.99817693]]\n",
            "Loss: 3.3235729564461322e-06\n",
            "Updated weights: [[0.10183133]\n",
            " [0.0904934 ]\n",
            " [0.2212486 ]]\n",
            "Updated biases: [[0.05215992]]\n",
            "\n",
            "Iteration 19:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.99872385]]\n",
            "Loss: 1.628550748658378e-06\n",
            "Updated weights: [[0.10185685]\n",
            " [0.09054445]\n",
            " [0.22132517]]\n",
            "Updated biases: [[0.05218544]]\n",
            "\n",
            "Iteration 20:\n",
            "Inputs: [[1. 2. 3.]]\n",
            "True output: [[1.]]\n",
            "Predicted output: [[0.9991067]]\n",
            "Loss: 7.979898668427639e-07\n",
            "Updated weights: [[0.10187472]\n",
            " [0.09058018]\n",
            " [0.22137877]]\n",
            "Updated biases: [[0.0522033]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our Loss is getting reduced and predicted output is close to True output,\n",
        "So our code is perfectly working."
      ],
      "metadata": {
        "id": "v7S5txKypySX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B1BRXQrbtTG-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Neural Network for 3 input features and 2 sample and trying to fit it in the model with output as 2 neuron"
      ],
      "metadata": {
        "id": "jmU0FSM_qP0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now for 2 samples\n",
        "np.random.seed(42)  # For reproducibility\n",
        "layer = Layer_Dense(n_inputs=3, n_neurons=2)\n",
        "print(\"Initial weights:\\n\", layer.weights)\n",
        "print(\"Initial biases:\\n\", layer.biases)\n",
        "\n",
        "inputs = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "y_true = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Training loop\n",
        "for i in range(20):  # Perform 5 iterations\n",
        "    # Forward pass\n",
        "    layer.forward(inputs)\n",
        "    y_pred = layer.output\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = mse_loss(y_pred, y_true)\n",
        "\n",
        "    # Compute the gradient of the loss with respect to the output\n",
        "    dvalues = mse_loss_gradient(y_pred, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    layer.backward(dvalues)\n",
        "\n",
        "    # Update weights and biases\n",
        "    layer.weights -= learning_rate * layer.dweights\n",
        "    layer.biases -= learning_rate * layer.dbiases\n",
        "\n",
        "    # Print the progress\n",
        "    print(f\"Iteration {i+1}:\")\n",
        "    print(\"Inputs:\", layer.inputs)\n",
        "    print(\"True Outputs:\", y_true)\n",
        "    print(\"Predicted output:\", y_pred)\n",
        "    print(\"Loss:\", loss)\n",
        "    print(\"Updated weights:\", layer.weights)\n",
        "    print(\"Updated biases:\", layer.biases)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH8M5EqgtbQ4",
        "outputId": "0cfccb3f-22eb-49d7-df8f-68e924cc717a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights:\n",
            " [[ 0.04967142 -0.01382643]\n",
            " [ 0.06476885  0.15230299]\n",
            " [-0.02341534 -0.0234137 ]]\n",
            "Initial biases:\n",
            " [[0. 0.]]\n",
            "Iteration 1:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[0.10896311 0.22053845]\n",
            " [0.38203791 0.56572703]]\n",
            "Loss: 5.65209661712092\n",
            "Updated weights: [[0.10648584 0.06375634]\n",
            " [0.13912828 0.25595443]\n",
            " [0.06848908 0.10630642]]\n",
            "Updated biases: [[0.01754499 0.02606867]]\n",
            "\n",
            "Iteration 2:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[0.60775462 0.92065311]\n",
            " [1.55006421 2.19870465]]\n",
            "Loss: 1.6664562244365677\n",
            "Updated weights: [[0.13744578 0.10517898]\n",
            " [0.17929912 0.31178028]\n",
            " [0.11787083 0.17653548]]\n",
            "Updated biases: [[0.0267559  0.04047188]]\n",
            "\n",
            "Iteration 3:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[0.87641243 1.29881786]\n",
            " [2.18025965 3.07930207]]\n",
            "Loss: 0.5066472978576841\n",
            "Updated weights: [[0.15445853 0.12709885]\n",
            " [0.20102851 0.34180955]\n",
            " [0.14431686 0.21467415]]\n",
            "Updated biases: [[0.03147254 0.04858128]]\n",
            "\n",
            "Iteration 4:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.02093866 1.50332168]\n",
            " [2.52035034 3.55406932]]\n",
            "Loss: 0.16901143875496594\n",
            "Updated weights: [[0.16394683 0.13850085]\n",
            " [0.21281036 0.3579246 ]\n",
            " [0.15839227 0.23550225]]\n",
            "Updated biases: [[0.0337661  0.05329433]]\n",
            "\n",
            "Iteration 5:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.09851045 1.61415112]\n",
            " [2.70395883 3.80993421]]\n",
            "Loss: 0.07058726305312399\n",
            "Updated weights: [[0.1693751  0.14423141]\n",
            " [0.21922629 0.36653473]\n",
            " [0.16579585 0.24699195]]\n",
            "Updated biases: [[0.03475375 0.0561739 ]]\n",
            "\n",
            "Iteration 6:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.13996896 1.67445064]\n",
            " [2.80316066 3.94772493]]\n",
            "Loss: 0.041763026360045724\n",
            "Updated weights: [[0.17261204 0.14690466]\n",
            " [0.22274758 0.3710971 ]\n",
            " [0.16960149 0.25344345]]\n",
            "Updated biases: [[0.0350381  0.05806302]]\n",
            "\n",
            "Iteration 7:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.16194978 1.70749223]\n",
            " [2.85683312 4.02182785]]\n",
            "Loss: 0.033190434551695285\n",
            "Updated weights: [[0.17466563 0.14793064]\n",
            " [0.22470726 0.37347648]\n",
            " [0.17146725 0.25717623]]\n",
            "Updated biases: [[0.03494419 0.05941642]]\n",
            "\n",
            "Iteration 8:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.17342608 1.72582871]\n",
            " [2.88594649 4.06157877]]\n",
            "Loss: 0.03051166190954023\n",
            "Updated weights: [[0.17607957 0.14806992]\n",
            " [0.22582433 0.37467873]\n",
            " [0.17228746 0.25944143]]\n",
            "Updated biases: [[0.03464732 0.06047939]]\n",
            "\n",
            "Iteration 9:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.17923795 1.73623106]\n",
            " [2.90181206 4.08280131]]\n",
            "Loss: 0.02954930650371523\n",
            "Updated weights: [[0.17714714 0.14773274]\n",
            " [0.22648665 0.37524638]\n",
            " [0.17254453 0.26091393]]\n",
            "Updated biases: [[0.03424207 0.06138422]]\n",
            "\n",
            "Iteration 10:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.18199612 1.74235152]\n",
            " [2.91053109 4.09403068]]\n",
            "Loss: 0.029087945376016767\n",
            "Updated weights: [[0.17802654 0.14714037]\n",
            " [0.22690341 0.3754721 ]\n",
            " [0.17249866 0.26195773]]\n",
            "Updated biases: [[0.03377944 0.06220231]]\n",
            "\n",
            "Iteration 11:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.18310878 1.74616009]\n",
            " [2.91539461 4.09987071]]\n",
            "Loss: 0.028773938977976546\n",
            "Updated weights: [[0.1788031  0.14641216]\n",
            " [0.22718746 0.37551373]\n",
            " [0.17229019 0.26276921]]\n",
            "Updated biases: [[0.03328692 0.06297216]]\n",
            "\n",
            "Iteration 12:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.18333551 1.74871942]\n",
            " [2.91817776 4.10280472]]\n",
            "Loss: 0.028504382459437157\n",
            "Updated weights: [[0.17952287 0.14561247]\n",
            " [0.22739966 0.37545642]\n",
            " [0.17199482 0.26345428]]\n",
            "Updated biases: [[0.03277935 0.06371454]]\n",
            "\n",
            "Iteration 13:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.18308602 1.75060268]\n",
            " [2.91983808 4.10417218]]\n",
            "Loss: 0.02824932197254345\n",
            "Updated weights: [[0.18021068 0.14477601]\n",
            " [0.22757285 0.37534609]\n",
            " [0.17165339 0.26407007]]\n",
            "Updated biases: [[0.03226473 0.06444067]]\n",
            "\n",
            "Iteration 14:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.18258128 1.75211907]\n",
            " [2.92089203 4.10469558]]\n",
            "Loss: 0.028000028774216226\n",
            "Updated weights: [[0.18087993 0.1439215 ]\n",
            " [0.22772474 0.37520751]\n",
            " [0.17128791 0.26464742]]\n",
            "Updated biases: [[0.03174737 0.06515659]]\n",
            "\n",
            "Iteration 15:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.1819405  1.75343537]\n",
            " [2.92161823 4.10476466]]\n",
            "Loss: 0.02775394965372944\n",
            "Updated weights: [[0.18153786 0.14305903]\n",
            " [0.22786487 0.37505404]\n",
            " [0.17091026 0.26520295]]\n",
            "Updated biases: [[0.03122957 0.06586559]]\n",
            "\n",
            "Iteration 16:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.18122796 1.75464155]\n",
            " [2.92216694 4.10458961]]\n",
            "Loss: 0.0275103282814061\n",
            "Updated weights: [[0.18218838 0.14219403]\n",
            " [0.22799842 0.37489288]\n",
            " [0.17052683 0.26574564]]\n",
            "Updated biases: [[0.0307126  0.06656944]]\n",
            "\n",
            "Iteration 17:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.18047831 1.75578615]\n",
            " [2.92261922 4.1042838 ]]\n",
            "Loss: 0.02726893124562107\n",
            "Updated weights: [[0.18283361 0.14132942]\n",
            " [0.22812816 0.37472793]\n",
            " [0.17014108 0.26628033]]\n",
            "Updated biases: [[0.03019711 0.06726909]]\n",
            "\n",
            "Iteration 18:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.17971027 1.75689536]\n",
            " [2.9230188  4.10390841]]\n",
            "Loss: 0.027029677394165064\n",
            "Updated weights: [[0.18347468 0.14046678]\n",
            " [0.22825559 0.37456126]\n",
            " [0.16975486 0.26680965]]\n",
            "Updated biases: [[0.02968347 0.06796507]]\n",
            "\n",
            "Iteration 19:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.1789339  1.75798332]\n",
            " [2.92338927 4.10349639]]\n",
            "Loss: 0.026792529990648073\n",
            "Updated weights: [[0.18411223 0.13960694]\n",
            " [0.22838151 0.37439402]\n",
            " [0.16936917 0.26733501]]\n",
            "Updated biases: [[0.02917185 0.06865767]]\n",
            "\n",
            "Iteration 20:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Predicted output: [[1.17815462 1.75905767]\n",
            " [2.92374336 4.10306555]]\n",
            "Loss: 0.026557465336549606\n",
            "Updated weights: [[0.18474659 0.13875034]\n",
            " [0.22850638 0.3742268 ]\n",
            " [0.16898455 0.26785718]]\n",
            "Updated biases: [[0.02866236 0.06934705]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our Loss is getting reduced and predicted output is close to True output,\n",
        "So our code is perfectly working."
      ],
      "metadata": {
        "id": "3H4LA40NqhjA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SV3apczlqnXq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for Dense Layer with ReLU activation"
      ],
      "metadata": {
        "id": "q9AFZmtpqmCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "3VIpSCUXzOWX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "metadata": {
        "id": "azWIxGtMyo5P"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum( 0 , inputs)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Since we need to modify original variable,\n",
        "    # let's make a copy of values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero gradient where input values were negative\n",
        "    self.dinputs[self.inputs <= 0 ] = 0"
      ],
      "metadata": {
        "id": "lHsegx8Vyoj_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-zloa9gygIU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Zd8_ysLt04m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qmrx8TxLyHe4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-E_6ufuqyMX5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PpEgJo70eqH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54LjVWwS0eml"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XPe6dCED0ejU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4Z7_lR10eLo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Dense Layer with ReLU activation for 3 input features and 2 sample and trying to fit it in the model with output as 2 neuron"
      ],
      "metadata": {
        "id": "_YQFClD1rAqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now including relu activation\n",
        "\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Initialize layers\n",
        "layer = Layer_Dense(n_inputs=3, n_neurons=2)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "print(\"Initial weights:\\n\", layer.weights)\n",
        "print(\"Initial biases:\\n\", layer.biases)\n",
        "\n",
        "\n",
        "inputs = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "y_true = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for i in range(20):  # Perform 20 iterations\n",
        "    # Forward pass\n",
        "    layer.forward(inputs)\n",
        "    layer_out = layer.output\n",
        "\n",
        "    activation1.forward(layer.output)\n",
        "    y_pred = activation1.output\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = mse_loss(y_pred, y_true)\n",
        "\n",
        "    # Compute the gradient of the loss with respect to the output\n",
        "    dvalues = mse_loss_gradient(y_pred, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "\n",
        "    activation1.backward(dvalues)\n",
        "    layer.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    layer.weights -= learning_rate * layer.dweights\n",
        "    layer.biases -= learning_rate * layer.dbiases\n",
        "\n",
        "    # Print the progress\n",
        "    print(f\"Iteration {i+1}:\")\n",
        "    print(\"Inputs:\", layer.inputs)\n",
        "    print(\"layer_out:\", layer_out)\n",
        "    print(\"True Outputs:\", y_true)\n",
        "    print(\"Activation/Predicted output:\", y_pred)\n",
        "    print(\"Loss:\", loss)\n",
        "    print(\"Updated weights:\", layer.weights)\n",
        "    print(\"Updated biases:\", layer.biases)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29y-CVO-yVi9",
        "outputId": "387a8cd4-4200-45e3-c865-70f9da0b61df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights:\n",
            " [[ 0.04967142 -0.01382643]\n",
            " [ 0.06476885  0.15230299]\n",
            " [-0.02341534 -0.0234137 ]]\n",
            "Initial biases:\n",
            " [[0. 0.]]\n",
            "Iteration 1:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[0.10896311 0.22053845]\n",
            " [0.38203791 0.56572703]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[0.10896311 0.22053845]\n",
            " [0.38203791 0.56572703]]\n",
            "Loss: 5.65209661712092\n",
            "Updated weights: [[0.10648584 0.06375634]\n",
            " [0.13912828 0.25595443]\n",
            " [0.06848908 0.10630642]]\n",
            "Updated biases: [[0.01754499 0.02606867]]\n",
            "\n",
            "Iteration 2:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[0.60775462 0.92065311]\n",
            " [1.55006421 2.19870465]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[0.60775462 0.92065311]\n",
            " [1.55006421 2.19870465]]\n",
            "Loss: 1.6664562244365677\n",
            "Updated weights: [[0.13744578 0.10517898]\n",
            " [0.17929912 0.31178028]\n",
            " [0.11787083 0.17653548]]\n",
            "Updated biases: [[0.0267559  0.04047188]]\n",
            "\n",
            "Iteration 3:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[0.87641243 1.29881786]\n",
            " [2.18025965 3.07930207]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[0.87641243 1.29881786]\n",
            " [2.18025965 3.07930207]]\n",
            "Loss: 0.5066472978576841\n",
            "Updated weights: [[0.15445853 0.12709885]\n",
            " [0.20102851 0.34180955]\n",
            " [0.14431686 0.21467415]]\n",
            "Updated biases: [[0.03147254 0.04858128]]\n",
            "\n",
            "Iteration 4:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.02093866 1.50332168]\n",
            " [2.52035034 3.55406932]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.02093866 1.50332168]\n",
            " [2.52035034 3.55406932]]\n",
            "Loss: 0.16901143875496594\n",
            "Updated weights: [[0.16394683 0.13850085]\n",
            " [0.21281036 0.3579246 ]\n",
            " [0.15839227 0.23550225]]\n",
            "Updated biases: [[0.0337661  0.05329433]]\n",
            "\n",
            "Iteration 5:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.09851045 1.61415112]\n",
            " [2.70395883 3.80993421]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.09851045 1.61415112]\n",
            " [2.70395883 3.80993421]]\n",
            "Loss: 0.07058726305312399\n",
            "Updated weights: [[0.1693751  0.14423141]\n",
            " [0.21922629 0.36653473]\n",
            " [0.16579585 0.24699195]]\n",
            "Updated biases: [[0.03475375 0.0561739 ]]\n",
            "\n",
            "Iteration 6:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.13996896 1.67445064]\n",
            " [2.80316066 3.94772493]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.13996896 1.67445064]\n",
            " [2.80316066 3.94772493]]\n",
            "Loss: 0.041763026360045724\n",
            "Updated weights: [[0.17261204 0.14690466]\n",
            " [0.22274758 0.3710971 ]\n",
            " [0.16960149 0.25344345]]\n",
            "Updated biases: [[0.0350381  0.05806302]]\n",
            "\n",
            "Iteration 7:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.16194978 1.70749223]\n",
            " [2.85683312 4.02182785]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.16194978 1.70749223]\n",
            " [2.85683312 4.02182785]]\n",
            "Loss: 0.033190434551695285\n",
            "Updated weights: [[0.17466563 0.14793064]\n",
            " [0.22470726 0.37347648]\n",
            " [0.17146725 0.25717623]]\n",
            "Updated biases: [[0.03494419 0.05941642]]\n",
            "\n",
            "Iteration 8:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.17342608 1.72582871]\n",
            " [2.88594649 4.06157877]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.17342608 1.72582871]\n",
            " [2.88594649 4.06157877]]\n",
            "Loss: 0.03051166190954023\n",
            "Updated weights: [[0.17607957 0.14806992]\n",
            " [0.22582433 0.37467873]\n",
            " [0.17228746 0.25944143]]\n",
            "Updated biases: [[0.03464732 0.06047939]]\n",
            "\n",
            "Iteration 9:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.17923795 1.73623106]\n",
            " [2.90181206 4.08280131]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.17923795 1.73623106]\n",
            " [2.90181206 4.08280131]]\n",
            "Loss: 0.02954930650371523\n",
            "Updated weights: [[0.17714714 0.14773274]\n",
            " [0.22648665 0.37524638]\n",
            " [0.17254453 0.26091393]]\n",
            "Updated biases: [[0.03424207 0.06138422]]\n",
            "\n",
            "Iteration 10:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.18199612 1.74235152]\n",
            " [2.91053109 4.09403068]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.18199612 1.74235152]\n",
            " [2.91053109 4.09403068]]\n",
            "Loss: 0.029087945376016767\n",
            "Updated weights: [[0.17802654 0.14714037]\n",
            " [0.22690341 0.3754721 ]\n",
            " [0.17249866 0.26195773]]\n",
            "Updated biases: [[0.03377944 0.06220231]]\n",
            "\n",
            "Iteration 11:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.18310878 1.74616009]\n",
            " [2.91539461 4.09987071]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.18310878 1.74616009]\n",
            " [2.91539461 4.09987071]]\n",
            "Loss: 0.028773938977976546\n",
            "Updated weights: [[0.1788031  0.14641216]\n",
            " [0.22718746 0.37551373]\n",
            " [0.17229019 0.26276921]]\n",
            "Updated biases: [[0.03328692 0.06297216]]\n",
            "\n",
            "Iteration 12:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.18333551 1.74871942]\n",
            " [2.91817776 4.10280472]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.18333551 1.74871942]\n",
            " [2.91817776 4.10280472]]\n",
            "Loss: 0.028504382459437157\n",
            "Updated weights: [[0.17952287 0.14561247]\n",
            " [0.22739966 0.37545642]\n",
            " [0.17199482 0.26345428]]\n",
            "Updated biases: [[0.03277935 0.06371454]]\n",
            "\n",
            "Iteration 13:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.18308602 1.75060268]\n",
            " [2.91983808 4.10417218]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.18308602 1.75060268]\n",
            " [2.91983808 4.10417218]]\n",
            "Loss: 0.02824932197254345\n",
            "Updated weights: [[0.18021068 0.14477601]\n",
            " [0.22757285 0.37534609]\n",
            " [0.17165339 0.26407007]]\n",
            "Updated biases: [[0.03226473 0.06444067]]\n",
            "\n",
            "Iteration 14:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.18258128 1.75211907]\n",
            " [2.92089203 4.10469558]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.18258128 1.75211907]\n",
            " [2.92089203 4.10469558]]\n",
            "Loss: 0.028000028774216226\n",
            "Updated weights: [[0.18087993 0.1439215 ]\n",
            " [0.22772474 0.37520751]\n",
            " [0.17128791 0.26464742]]\n",
            "Updated biases: [[0.03174737 0.06515659]]\n",
            "\n",
            "Iteration 15:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.1819405  1.75343537]\n",
            " [2.92161823 4.10476466]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.1819405  1.75343537]\n",
            " [2.92161823 4.10476466]]\n",
            "Loss: 0.02775394965372944\n",
            "Updated weights: [[0.18153786 0.14305903]\n",
            " [0.22786487 0.37505404]\n",
            " [0.17091026 0.26520295]]\n",
            "Updated biases: [[0.03122957 0.06586559]]\n",
            "\n",
            "Iteration 16:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.18122796 1.75464155]\n",
            " [2.92216694 4.10458961]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.18122796 1.75464155]\n",
            " [2.92216694 4.10458961]]\n",
            "Loss: 0.0275103282814061\n",
            "Updated weights: [[0.18218838 0.14219403]\n",
            " [0.22799842 0.37489288]\n",
            " [0.17052683 0.26574564]]\n",
            "Updated biases: [[0.0307126  0.06656944]]\n",
            "\n",
            "Iteration 17:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.18047831 1.75578615]\n",
            " [2.92261922 4.1042838 ]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.18047831 1.75578615]\n",
            " [2.92261922 4.1042838 ]]\n",
            "Loss: 0.02726893124562107\n",
            "Updated weights: [[0.18283361 0.14132942]\n",
            " [0.22812816 0.37472793]\n",
            " [0.17014108 0.26628033]]\n",
            "Updated biases: [[0.03019711 0.06726909]]\n",
            "\n",
            "Iteration 18:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.17971027 1.75689536]\n",
            " [2.9230188  4.10390841]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.17971027 1.75689536]\n",
            " [2.9230188  4.10390841]]\n",
            "Loss: 0.027029677394165064\n",
            "Updated weights: [[0.18347468 0.14046678]\n",
            " [0.22825559 0.37456126]\n",
            " [0.16975486 0.26680965]]\n",
            "Updated biases: [[0.02968347 0.06796507]]\n",
            "\n",
            "Iteration 19:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.1789339  1.75798332]\n",
            " [2.92338927 4.10349639]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.1789339  1.75798332]\n",
            " [2.92338927 4.10349639]]\n",
            "Loss: 0.026792529990648073\n",
            "Updated weights: [[0.18411223 0.13960694]\n",
            " [0.22838151 0.37439402]\n",
            " [0.16936917 0.26733501]]\n",
            "Updated biases: [[0.02917185 0.06865767]]\n",
            "\n",
            "Iteration 20:\n",
            "Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "layer_out: [[1.17815462 1.75905767]\n",
            " [2.92374336 4.10306555]]\n",
            "True Outputs: [[1. 2.]\n",
            " [3. 4.]]\n",
            "Activation/Predicted output: [[1.17815462 1.75905767]\n",
            " [2.92374336 4.10306555]]\n",
            "Loss: 0.026557465336549606\n",
            "Updated weights: [[0.18474659 0.13875034]\n",
            " [0.22850638 0.3742268 ]\n",
            " [0.16898455 0.26785718]]\n",
            "Updated biases: [[0.02866236 0.06934705]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our Loss is getting reduced and predicted output is close to True output,\n",
        "So our code is perfectly working."
      ],
      "metadata": {
        "id": "UrG5IQMLrOJT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDSjueUQ135U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for Dense Layer with SoftMax activation."
      ],
      "metadata": {
        "id": "HlvSNTkSrUaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now for Softmax Activation\n",
        "import numpy as np\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "\n",
        "class Activation_Softmax:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Subtract max value for numerical stability\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        # Normalize probabilities\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output and its gradient\n",
        "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n"
      ],
      "metadata": {
        "id": "MeZ2My555VYm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Cross-Entropy Loss\n",
        "def categorical_crossentropy_loss(y_pred, y_true):\n",
        "    # Clip data to prevent division by 0\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    # Calculate loss\n",
        "    correct_confidences = y_pred[range(len(y_pred)), y_true]\n",
        "    return -np.mean(np.log(correct_confidences))\n",
        "\n",
        "\n",
        "# Gradient of Categorical Cross-Entropy Loss\n",
        "def categorical_crossentropy_loss_gradient(y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    labels = len(y_pred[0])\n",
        "\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Initialize the gradient\n",
        "    gradients = np.zeros_like(y_pred)\n",
        "\n",
        "    # Convert labels to one-hot if they are not\n",
        "    if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]\n",
        "\n",
        "    # Calculate gradient\n",
        "    gradients = -y_true / y_pred\n",
        "    gradients = gradients / samples\n",
        "\n",
        "    return gradients\n"
      ],
      "metadata": {
        "id": "PjfOKuX_5VVB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7RVgsiB7eDL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Dense Layer with SoftMax activation for 3 input features and 2 sample and trying to fit it in the model with output as 2 neuron"
      ],
      "metadata": {
        "id": "ItpxAcnzrj7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Initialize layers\n",
        "dense1 = Layer_Dense(n_inputs=3, n_neurons=2)\n",
        "activation1 = Activation_Softmax()\n",
        "\n",
        "print(\"Initial weights:\\n\", dense1.weights)\n",
        "print(\"Initial biases:\\n\", dense1.biases)\n",
        "\n",
        "inputs = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "y_true = np.array([0, 1])\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for i in range(10):  # Perform 5 iterations\n",
        "\n",
        "    # Forward pass through dense layer\n",
        "    dense1.forward(inputs)\n",
        "\n",
        "    # Forward pass through softmax activation layer\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    y_pred = activation1.output\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = categorical_crossentropy_loss(activation1.output, y_true)\n",
        "\n",
        "    # Compute the gradient of the loss with respect to the output\n",
        "    dvalues = categorical_crossentropy_loss_gradient(activation1.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    activation1.backward(dvalues)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    dense1.weights -= learning_rate * dense1.dweights\n",
        "    dense1.biases -= learning_rate * dense1.dbiases\n",
        "\n",
        "    # Print the progress\n",
        "    print(f\"Iteration {i+1}:\")\n",
        "    print(\"Dense layer Inputs:\", dense1.inputs)\n",
        "    print(\"Dense layer output:\", dense1.output)\n",
        "    print(\"True Outputs:\", y_true)\n",
        "    print(\"Softmax Activation/Predicted output:\", y_pred)\n",
        "    print(\"Loss:\", loss)\n",
        "    print(\"Updated weights:\", dense1.weights)\n",
        "    print(\"Updated biases:\", dense1.biases)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "BdUUMDJm7giz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef62d7b-ac39-45e6-fede-1b5d623d9c21"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights:\n",
            " [[ 0.04967142 -0.01382643]\n",
            " [ 0.06476885  0.15230299]\n",
            " [-0.02341534 -0.0234137 ]]\n",
            "Initial biases:\n",
            " [[0. 0.]]\n",
            "Iteration 1:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[0.10896311 0.22053845]\n",
            " [0.38203791 0.56572703]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.47213507 0.52786493]\n",
            " [0.45420641 0.54579359]]\n",
            "Loss: 0.6780022950324187\n",
            "Updated weights: [[-0.01477662  0.0506216 ]\n",
            " [ 0.00400375  0.21306809]\n",
            " [-0.08049752  0.03366849]]\n",
            "Updated biases: [[ 0.00368293 -0.00368293]]\n",
            "\n",
            "Iteration 2:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[-0.24457876  0.57408033]\n",
            " [-0.51838995  1.46615488]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.30604837 0.69395163]\n",
            " [0.12083519 0.87916481]]\n",
            "Loss: 0.6563975061565652\n",
            "Updated weights: [[-0.00424608  0.04009106]\n",
            " [ 0.04319011  0.17388173]\n",
            " [-0.01265533 -0.0341737 ]]\n",
            "Updated biases: [[ 0.03233875 -0.03233875]]\n",
            "\n",
            "Iteration 3:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[0.0765069  0.25299467]\n",
            " [0.155373   0.79239194]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.45599223 0.54400777]\n",
            " [0.34592072 0.65407928]]\n",
            "Loss: 0.6049031165919792\n",
            "Updated weights: [[-0.04622983  0.08207482]\n",
            " [ 0.01111071  0.20596113]\n",
            " [-0.03483038 -0.01199865]]\n",
            "Updated biases: [[ 0.0422431 -0.0422431]]\n",
            "\n",
            "Iteration 4:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[-0.08625647  0.41575803]\n",
            " [-0.29610499  1.24386993]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.37706737 0.62293263]\n",
            " [0.17653892 0.82346108]]\n",
            "Loss: 0.5847851986983869\n",
            "Updated weights: [[-0.05039098  0.08623597]\n",
            " [ 0.02926924  0.1878026 ]\n",
            " [ 0.00564783 -0.05247687]]\n",
            "Updated biases: [[ 0.06456279 -0.06456279]]\n",
            "\n",
            "Iteration 5:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[0.08965379 0.23984778]\n",
            " [0.04323206 0.90453288]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.46252193 0.53747807]\n",
            " [0.29706764 0.70293236]]\n",
            "Loss: 0.5617779572110506\n",
            "Updated weights: [[-0.08293061  0.11877559]\n",
            " [ 0.00875014  0.2083217 ]\n",
            " [-0.00285075 -0.04397829]]\n",
            "Updated biases: [[ 0.07658331 -0.07658331]]\n",
            "\n",
            "Iteration 6:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[ 0.00260074  0.32690083]\n",
            " [-0.22849291  1.17625785]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.41962814 0.58037186]\n",
            " [0.19706332 0.80293668]]\n",
            "Loss: 0.5439328786964668\n",
            "Updated weights: [[-0.09332468  0.12916966]\n",
            " [ 0.01752149  0.19955035]\n",
            " [ 0.02508604 -0.07191507]]\n",
            "Updated biases: [[ 0.09574873 -0.09574873]]\n",
            "\n",
            "Iteration 7:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[ 0.11272515  0.21677641]\n",
            " [-0.03942629  0.98719123]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.47401063 0.52598937]\n",
            " [0.26374039 0.73625961]]\n",
            "Loss: 0.5263490137510657\n",
            "Updated weights: [[-0.11977329  0.15561827]\n",
            " [ 0.00418533  0.21288651]\n",
            " [ 0.02486232 -0.07169136]]\n",
            "Updated biases: [[ 0.10886118 -0.10886118]]\n",
            "\n",
            "Iteration 8:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[ 0.07204553  0.25745603]\n",
            " [-0.20013137  1.1478963 ]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.45377971 0.54622029]\n",
            " [0.20619301 0.79380699]]\n",
            "Loss: 0.5105291772994118\n",
            "Updated weights: [[-0.13370088  0.16954586]\n",
            " [ 0.00725911  0.20981273]\n",
            " [ 0.04493747 -0.0917665 ]]\n",
            "Updated biases: [[ 0.12586255 -0.12586255]]\n",
            "\n",
            "Iteration 9:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[ 0.14149228  0.18800928]\n",
            " [-0.10302063  1.05078557]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.48837285 0.51162715]\n",
            " [0.23979455 0.76020545]]\n",
            "Loss: 0.49542134578901187\n",
            "Updated weights: [[-0.15607843  0.19192342]\n",
            " [-0.00152681  0.21859865]\n",
            " [ 0.04974317 -0.09657221]]\n",
            "Updated biases: [[ 0.13945418 -0.13945418]]\n",
            "\n",
            "Iteration 10:\n",
            "Dense layer Inputs: [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "Dense layer output: [[ 0.12955164  0.19994993]\n",
            " [-0.19403458  1.14179952]]\n",
            "True Outputs: [0 1]\n",
            "Softmax Activation/Predicted output: [[0.48240769 0.51759231]\n",
            " [0.20819597 0.79180403]]\n",
            "Loss: 0.48120352442640285\n",
            "Updated weights: [[-0.17183801  0.20768299]\n",
            " [-0.00181658  0.21888842]\n",
            " [ 0.06492323 -0.11175226]]\n",
            "Updated biases: [[ 0.15492399 -0.15492399]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our Loss is getting reduced and predicted output is close to True output,\n",
        "So our code is perfectly working."
      ],
      "metadata": {
        "id": "s1p-L94jsOJu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pj7ckUXjmz2a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Code upto this point"
      ],
      "metadata": {
        "id": "0hajfTPYEcBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Layer_Dense :\n",
        "  # Layer initialization\n",
        "  def __init__ ( self , n_inputs , n_neurons ):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros(( 1 , n_neurons))\n",
        "\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
        "    # Gradient on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# Mean Squared Error Loss and its gradient\n",
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def mse_loss_gradient(y_pred, y_true):\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU :\n",
        "  # Forward pass\n",
        "  def forward ( self , inputs ):\n",
        "    # Remember input values\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs\n",
        "    self.output = np.maximum( 0 , inputs)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward ( self , dvalues ):\n",
        "    # Since we need to modify original variable,\n",
        "    # let's make a copy of values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Zero gradient where input values were negative\n",
        "    self.dinputs[self.inputs <= 0 ] = 0\n",
        "\n",
        "################################################################################\n",
        "\n",
        "class Activation_Softmax:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Subtract max value for numerical stability\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        # Normalize probabilities\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output and its gradient\n",
        "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# Categorical Cross-Entropy Loss\n",
        "def categorical_crossentropy_loss(y_pred, y_true):\n",
        "    # Clip data to prevent division by 0\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    # Calculate loss\n",
        "    correct_confidences = y_pred[range(len(y_pred)), y_true]\n",
        "    return -np.mean(np.log(correct_confidences))\n",
        "\n",
        "\n",
        "# Gradient of Categorical Cross-Entropy Loss\n",
        "def categorical_crossentropy_loss_gradient(y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    labels = len(y_pred[0])\n",
        "\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Initialize the gradient\n",
        "    gradients = np.zeros_like(y_pred)\n",
        "\n",
        "    # Convert labels to one-hot if they are not\n",
        "    if len(y_true.shape) == 1:\n",
        "        y_true = np.eye(labels)[y_true]\n",
        "\n",
        "    # Calculate gradient\n",
        "    gradients = -y_true / y_pred\n",
        "    gradients = gradients / samples\n",
        "\n",
        "    return gradients\n"
      ],
      "metadata": {
        "id": "Tyr8t56pmzt_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "88rq1Awi9a_-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4BlZSjTz9O1",
        "outputId": "4392dc9b-06fa-4dbf-990f-352e1d084b56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from nnfs.datasets import spiral_data\n",
        "import matplotlib.pyplot as plt\n",
        "import nnfs\n",
        "from nnfs.datasets import vertical_data\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "GLMeW7VHB04D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eMXzOgdz0sR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing some test Code with optimizer"
      ],
      "metadata": {
        "id": "nyr6D-BtEqYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = spiral_data( samples = 100 , classes = 3 )"
      ],
      "metadata": {
        "id": "1yPDAjKZwnHM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV7Qf8rhjja-",
        "outputId": "513dbc44-e2ed-4840-a63c-fc24802111f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 2)\n",
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBRFMElmkYT7",
        "outputId": "7c98a530-90a8-497c-8a7c-2e9981bd016b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In normal optimizer loss is decreasing slowly and also accuracy is also increasing slowly"
      ],
      "metadata": {
        "id": "SSVZVZ8wFLxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common SGD vanilla Optimizer"
      ],
      "metadata": {
        "id": "axrqIWS-Fyka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## optimizers\n",
        "#Common SGD Vanilla optimizer\n",
        "class Optimizer_SGD :\n",
        "\n",
        "  # Initialize optimizer - set settings,\n",
        "  # learning rate of 1. is default for this optimizer\n",
        "  def __init__ ( self , learning_rate = 1.0, decay = 0.):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "\n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params ( self ):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate *( 1. / ( 1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params ( self , layer ):\n",
        "\n",
        "    layer.weights += - self.current_learning_rate * layer.dweights\n",
        "    layer.biases += - self.current_learning_rate * layer.dbiases\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params ( self ):\n",
        "    self.iterations += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "iKVWX7vqUXiB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(2, 64)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense( 64 , 3 )\n",
        "activation2 = Activation_Softmax()\n",
        "# loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(learning_rate = 1, decay = 1e-3)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range ( 10001 ):\n",
        "\n",
        "    ##training\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    loss = categorical_crossentropy_loss(activation2.output, y)\n",
        "    loss_grad = categorical_crossentropy_loss_gradient(activation2.output, y) #dvalues\n",
        "\n",
        "    # loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    predictions = np.argmax(activation2.output, axis = 1 )\n",
        "    if len (y.shape) == 2 :\n",
        "      y = np.argmax(y, axis = 1 )\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    if not epoch % 100 :\n",
        "      print(f\"epoch:{epoch} ,acc:{accuracy:.3f} ,loss:{loss:.3f} ,lr:{optimizer.current_learning_rate}\")\n",
        "\n",
        "\n",
        "    # # Backward pass\n",
        "    activation2.backward(loss_grad)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Print gradients\n",
        "    # print (dense1.dweights)\n",
        "    # print (dense1.dbiases)\n",
        "    # print (dense2.dweights)\n",
        "    # print (dense2.dbiases)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()"
      ],
      "metadata": {
        "id": "M957iFykUMTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd76733-6bc6-411c-e347-49fa3b80658a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 ,acc:0.270 ,loss:1.102 ,lr:1\n",
            "epoch:100 ,acc:0.403 ,loss:1.076 ,lr:0.9099181073703367\n",
            "epoch:200 ,acc:0.410 ,loss:1.072 ,lr:0.8340283569641367\n",
            "epoch:300 ,acc:0.423 ,loss:1.068 ,lr:0.7698229407236336\n",
            "epoch:400 ,acc:0.420 ,loss:1.064 ,lr:0.7147962830593281\n",
            "epoch:500 ,acc:0.413 ,loss:1.059 ,lr:0.66711140760507\n",
            "epoch:600 ,acc:0.413 ,loss:1.053 ,lr:0.6253908692933083\n",
            "epoch:700 ,acc:0.420 ,loss:1.046 ,lr:0.5885815185403178\n",
            "epoch:800 ,acc:0.430 ,loss:1.040 ,lr:0.5558643690939411\n",
            "epoch:900 ,acc:0.437 ,loss:1.032 ,lr:0.526592943654555\n",
            "epoch:1000 ,acc:0.440 ,loss:1.023 ,lr:0.5002501250625312\n",
            "epoch:1100 ,acc:0.463 ,loss:1.014 ,lr:0.4764173415912339\n",
            "epoch:1200 ,acc:0.513 ,loss:1.003 ,lr:0.45475216007276037\n",
            "epoch:1300 ,acc:0.537 ,loss:0.993 ,lr:0.43497172683775553\n",
            "epoch:1400 ,acc:0.553 ,loss:0.983 ,lr:0.4168403501458941\n",
            "epoch:1500 ,acc:0.573 ,loss:0.983 ,lr:0.4001600640256102\n",
            "epoch:1600 ,acc:0.580 ,loss:0.982 ,lr:0.3847633705271258\n",
            "epoch:1700 ,acc:0.597 ,loss:0.975 ,lr:0.3705075954057058\n",
            "epoch:1800 ,acc:0.593 ,loss:0.969 ,lr:0.35727045373347627\n",
            "epoch:1900 ,acc:0.590 ,loss:0.964 ,lr:0.3449465332873405\n",
            "epoch:2000 ,acc:0.597 ,loss:0.959 ,lr:0.33344448149383127\n",
            "epoch:2100 ,acc:0.600 ,loss:0.952 ,lr:0.32268473701193934\n",
            "epoch:2200 ,acc:0.597 ,loss:0.946 ,lr:0.31259768677711786\n",
            "epoch:2300 ,acc:0.593 ,loss:0.942 ,lr:0.3031221582297666\n",
            "epoch:2400 ,acc:0.580 ,loss:0.937 ,lr:0.29420417769932333\n",
            "epoch:2500 ,acc:0.577 ,loss:0.931 ,lr:0.2857959416976279\n",
            "epoch:2600 ,acc:0.577 ,loss:0.927 ,lr:0.2778549597110308\n",
            "epoch:2700 ,acc:0.580 ,loss:0.922 ,lr:0.2703433360367667\n",
            "epoch:2800 ,acc:0.573 ,loss:0.917 ,lr:0.26322716504343247\n",
            "epoch:2900 ,acc:0.573 ,loss:0.913 ,lr:0.25647601949217746\n",
            "epoch:3000 ,acc:0.580 ,loss:0.909 ,lr:0.25006251562890724\n",
            "epoch:3100 ,acc:0.583 ,loss:0.904 ,lr:0.2439619419370578\n",
            "epoch:3200 ,acc:0.583 ,loss:0.898 ,lr:0.23815194093831865\n",
            "epoch:3300 ,acc:0.597 ,loss:0.894 ,lr:0.23261223540358225\n",
            "epoch:3400 ,acc:0.597 ,loss:0.889 ,lr:0.22732439190725165\n",
            "epoch:3500 ,acc:0.613 ,loss:0.885 ,lr:0.22227161591464767\n",
            "epoch:3600 ,acc:0.597 ,loss:0.880 ,lr:0.21743857360295715\n",
            "epoch:3700 ,acc:0.620 ,loss:0.877 ,lr:0.21281123643328367\n",
            "epoch:3800 ,acc:0.597 ,loss:0.880 ,lr:0.20837674515524068\n",
            "epoch:3900 ,acc:0.603 ,loss:0.872 ,lr:0.20412329046744235\n",
            "epoch:4000 ,acc:0.657 ,loss:0.866 ,lr:0.2000400080016003\n",
            "epoch:4100 ,acc:0.607 ,loss:0.865 ,lr:0.19611688566385566\n",
            "epoch:4200 ,acc:0.607 ,loss:0.860 ,lr:0.19234468166955185\n",
            "epoch:4300 ,acc:0.657 ,loss:0.854 ,lr:0.18871485185884126\n",
            "epoch:4400 ,acc:0.603 ,loss:0.855 ,lr:0.18521948508983144\n",
            "epoch:4500 ,acc:0.623 ,loss:0.849 ,lr:0.18185124568103292\n",
            "epoch:4600 ,acc:0.617 ,loss:0.845 ,lr:0.1786033220217896\n",
            "epoch:4700 ,acc:0.617 ,loss:0.842 ,lr:0.1754693805930865\n",
            "epoch:4800 ,acc:0.623 ,loss:0.838 ,lr:0.17244352474564578\n",
            "epoch:4900 ,acc:0.623 ,loss:0.835 ,lr:0.16952025767079165\n",
            "epoch:5000 ,acc:0.623 ,loss:0.832 ,lr:0.16669444907484582\n",
            "epoch:5100 ,acc:0.630 ,loss:0.829 ,lr:0.16396130513198884\n",
            "epoch:5200 ,acc:0.623 ,loss:0.825 ,lr:0.16131634134537828\n",
            "epoch:5300 ,acc:0.637 ,loss:0.822 ,lr:0.15875535799333226\n",
            "epoch:5400 ,acc:0.633 ,loss:0.819 ,lr:0.1562744178777934\n",
            "epoch:5500 ,acc:0.637 ,loss:0.817 ,lr:0.15386982612709646\n",
            "epoch:5600 ,acc:0.643 ,loss:0.813 ,lr:0.15153811183512653\n",
            "epoch:5700 ,acc:0.640 ,loss:0.810 ,lr:0.14927601134497687\n",
            "epoch:5800 ,acc:0.647 ,loss:0.807 ,lr:0.14708045300779526\n",
            "epoch:5900 ,acc:0.650 ,loss:0.804 ,lr:0.14494854326714016\n",
            "epoch:6000 ,acc:0.653 ,loss:0.801 ,lr:0.1428775539362766\n",
            "epoch:6100 ,acc:0.653 ,loss:0.798 ,lr:0.1408649105507818\n",
            "epoch:6200 ,acc:0.653 ,loss:0.794 ,lr:0.13890818169190167\n",
            "epoch:6300 ,acc:0.657 ,loss:0.792 ,lr:0.13700506918755992\n",
            "epoch:6400 ,acc:0.660 ,loss:0.789 ,lr:0.13515339910798757\n",
            "epoch:6500 ,acc:0.657 ,loss:0.787 ,lr:0.13335111348179757\n",
            "epoch:6600 ,acc:0.660 ,loss:0.783 ,lr:0.13159626266614027\n",
            "epoch:6700 ,acc:0.663 ,loss:0.780 ,lr:0.12988699831146902\n",
            "epoch:6800 ,acc:0.667 ,loss:0.777 ,lr:0.12822156686754713\n",
            "epoch:6900 ,acc:0.667 ,loss:0.775 ,lr:0.126598303582732\n",
            "epoch:7000 ,acc:0.673 ,loss:0.772 ,lr:0.12501562695336915\n",
            "epoch:7100 ,acc:0.673 ,loss:0.770 ,lr:0.12347203358439313\n",
            "epoch:7200 ,acc:0.670 ,loss:0.768 ,lr:0.12196609342602757\n",
            "epoch:7300 ,acc:0.670 ,loss:0.765 ,lr:0.12049644535486204\n",
            "epoch:7400 ,acc:0.677 ,loss:0.762 ,lr:0.11906179307060363\n",
            "epoch:7500 ,acc:0.683 ,loss:0.760 ,lr:0.11766090128250381\n",
            "epoch:7600 ,acc:0.683 ,loss:0.758 ,lr:0.11629259216187929\n",
            "epoch:7700 ,acc:0.687 ,loss:0.756 ,lr:0.11495574203931487\n",
            "epoch:7800 ,acc:0.687 ,loss:0.753 ,lr:0.11364927832708263\n",
            "epoch:7900 ,acc:0.690 ,loss:0.751 ,lr:0.11237217664906168\n",
            "epoch:8000 ,acc:0.703 ,loss:0.748 ,lr:0.11112345816201799\n",
            "epoch:8100 ,acc:0.707 ,loss:0.746 ,lr:0.10990218705352237\n",
            "epoch:8200 ,acc:0.707 ,loss:0.743 ,lr:0.10870746820306555\n",
            "epoch:8300 ,acc:0.707 ,loss:0.742 ,lr:0.1075384449940854\n",
            "epoch:8400 ,acc:0.707 ,loss:0.740 ,lr:0.10639429726566654\n",
            "epoch:8500 ,acc:0.707 ,loss:0.737 ,lr:0.10527423939362038\n",
            "epoch:8600 ,acc:0.707 ,loss:0.735 ,lr:0.10417751849150952\n",
            "epoch:8700 ,acc:0.710 ,loss:0.733 ,lr:0.10310341272296113\n",
            "epoch:8800 ,acc:0.720 ,loss:0.730 ,lr:0.1020512297173181\n",
            "epoch:8900 ,acc:0.717 ,loss:0.729 ,lr:0.10102030508132134\n",
            "epoch:9000 ,acc:0.720 ,loss:0.727 ,lr:0.1000100010001\n",
            "epoch:9100 ,acc:0.720 ,loss:0.725 ,lr:0.09901970492127933\n",
            "epoch:9200 ,acc:0.730 ,loss:0.722 ,lr:0.09804882831650162\n",
            "epoch:9300 ,acc:0.727 ,loss:0.720 ,lr:0.09709680551509856\n",
            "epoch:9400 ,acc:0.727 ,loss:0.718 ,lr:0.09616309260505818\n",
            "epoch:9500 ,acc:0.723 ,loss:0.716 ,lr:0.09524716639679968\n",
            "epoch:9600 ,acc:0.723 ,loss:0.715 ,lr:0.09434852344560807\n",
            "epoch:9700 ,acc:0.727 ,loss:0.713 ,lr:0.09346667912889055\n",
            "epoch:9800 ,acc:0.737 ,loss:0.711 ,lr:0.09260116677470137\n",
            "epoch:9900 ,acc:0.733 ,loss:0.710 ,lr:0.09175153683824203\n",
            "epoch:10000 ,acc:0.740 ,loss:0.708 ,lr:0.09091735612328393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In normal Vanilla optimizer loss is decreasing slowly and also accuracy is also increasing slowly"
      ],
      "metadata": {
        "id": "Oh_aongJFirb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QO7IJfcjUMOr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0u23AwwnUMLL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD optimizer with Momentum and Vanilla"
      ],
      "metadata": {
        "id": "BuKMeT0dF_Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer with momentum and vanilla\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "M97kFuIVYQhI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(2, 64)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense( 64 , 3 )\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_SGD(learning_rate = 1, decay = 1e-3, momentum = 0.5)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range ( 10001 ):\n",
        "\n",
        "    ##training\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    loss = categorical_crossentropy_loss(activation2.output, y)\n",
        "    loss_grad = categorical_crossentropy_loss_gradient(activation2.output, y) #dvalues\n",
        "\n",
        "    # loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    predictions = np.argmax(activation2.output, axis = 1 )\n",
        "    if len (y.shape) == 2 :\n",
        "      y = np.argmax(y, axis = 1 )\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    if not epoch % 100 :\n",
        "      print(f\"epoch:{epoch} ,acc:{accuracy:.3f} ,loss:{loss:.3f} ,lr:{optimizer.current_learning_rate}\")\n",
        "\n",
        "\n",
        "    # # Backward pass\n",
        "    activation2.backward(loss_grad)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Print gradients\n",
        "    # print (dense1.dweights)\n",
        "    # print (dense1.dbiases)\n",
        "    # print (dense2.dweights)\n",
        "    # print (dense2.dbiases)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()"
      ],
      "metadata": {
        "id": "VDL86OmLYQL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47d9225-10ec-4286-e21b-b3886cee7b91"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 ,acc:0.307 ,loss:1.098 ,lr:1\n",
            "epoch:100 ,acc:0.410 ,loss:1.070 ,lr:0.9099181073703367\n",
            "epoch:200 ,acc:0.430 ,loss:1.061 ,lr:0.8340283569641367\n",
            "epoch:300 ,acc:0.450 ,loss:1.044 ,lr:0.7698229407236336\n",
            "epoch:400 ,acc:0.463 ,loss:1.024 ,lr:0.7147962830593281\n",
            "epoch:500 ,acc:0.553 ,loss:0.995 ,lr:0.66711140760507\n",
            "epoch:600 ,acc:0.500 ,loss:0.991 ,lr:0.6253908692933083\n",
            "epoch:700 ,acc:0.553 ,loss:0.960 ,lr:0.5885815185403178\n",
            "epoch:800 ,acc:0.573 ,loss:0.919 ,lr:0.5558643690939411\n",
            "epoch:900 ,acc:0.587 ,loss:0.882 ,lr:0.526592943654555\n",
            "epoch:1000 ,acc:0.633 ,loss:0.847 ,lr:0.5002501250625312\n",
            "epoch:1100 ,acc:0.603 ,loss:0.831 ,lr:0.4764173415912339\n",
            "epoch:1200 ,acc:0.700 ,loss:0.791 ,lr:0.45475216007276037\n",
            "epoch:1300 ,acc:0.687 ,loss:0.785 ,lr:0.43497172683775553\n",
            "epoch:1400 ,acc:0.653 ,loss:0.750 ,lr:0.4168403501458941\n",
            "epoch:1500 ,acc:0.633 ,loss:0.742 ,lr:0.4001600640256102\n",
            "epoch:1600 ,acc:0.710 ,loss:0.719 ,lr:0.3847633705271258\n",
            "epoch:1700 ,acc:0.660 ,loss:0.705 ,lr:0.3705075954057058\n",
            "epoch:1800 ,acc:0.703 ,loss:0.696 ,lr:0.35727045373347627\n",
            "epoch:1900 ,acc:0.690 ,loss:0.692 ,lr:0.3449465332873405\n",
            "epoch:2000 ,acc:0.703 ,loss:0.671 ,lr:0.33344448149383127\n",
            "epoch:2100 ,acc:0.713 ,loss:0.660 ,lr:0.32268473701193934\n",
            "epoch:2200 ,acc:0.723 ,loss:0.652 ,lr:0.31259768677711786\n",
            "epoch:2300 ,acc:0.723 ,loss:0.640 ,lr:0.3031221582297666\n",
            "epoch:2400 ,acc:0.737 ,loss:0.632 ,lr:0.29420417769932333\n",
            "epoch:2500 ,acc:0.737 ,loss:0.625 ,lr:0.2857959416976279\n",
            "epoch:2600 ,acc:0.763 ,loss:0.612 ,lr:0.2778549597110308\n",
            "epoch:2700 ,acc:0.767 ,loss:0.602 ,lr:0.2703433360367667\n",
            "epoch:2800 ,acc:0.757 ,loss:0.598 ,lr:0.26322716504343247\n",
            "epoch:2900 ,acc:0.757 ,loss:0.587 ,lr:0.25647601949217746\n",
            "epoch:3000 ,acc:0.767 ,loss:0.579 ,lr:0.25006251562890724\n",
            "epoch:3100 ,acc:0.773 ,loss:0.570 ,lr:0.2439619419370578\n",
            "epoch:3200 ,acc:0.777 ,loss:0.566 ,lr:0.23815194093831865\n",
            "epoch:3300 ,acc:0.770 ,loss:0.561 ,lr:0.23261223540358225\n",
            "epoch:3400 ,acc:0.773 ,loss:0.555 ,lr:0.22732439190725165\n",
            "epoch:3500 ,acc:0.773 ,loss:0.551 ,lr:0.22227161591464767\n",
            "epoch:3600 ,acc:0.780 ,loss:0.546 ,lr:0.21743857360295715\n",
            "epoch:3700 ,acc:0.783 ,loss:0.541 ,lr:0.21281123643328367\n",
            "epoch:3800 ,acc:0.790 ,loss:0.537 ,lr:0.20837674515524068\n",
            "epoch:3900 ,acc:0.797 ,loss:0.533 ,lr:0.20412329046744235\n",
            "epoch:4000 ,acc:0.793 ,loss:0.530 ,lr:0.2000400080016003\n",
            "epoch:4100 ,acc:0.797 ,loss:0.526 ,lr:0.19611688566385566\n",
            "epoch:4200 ,acc:0.800 ,loss:0.523 ,lr:0.19234468166955185\n",
            "epoch:4300 ,acc:0.810 ,loss:0.519 ,lr:0.18871485185884126\n",
            "epoch:4400 ,acc:0.810 ,loss:0.516 ,lr:0.18521948508983144\n",
            "epoch:4500 ,acc:0.803 ,loss:0.513 ,lr:0.18185124568103292\n",
            "epoch:4600 ,acc:0.810 ,loss:0.510 ,lr:0.1786033220217896\n",
            "epoch:4700 ,acc:0.813 ,loss:0.506 ,lr:0.1754693805930865\n",
            "epoch:4800 ,acc:0.813 ,loss:0.503 ,lr:0.17244352474564578\n",
            "epoch:4900 ,acc:0.817 ,loss:0.500 ,lr:0.16952025767079165\n",
            "epoch:5000 ,acc:0.817 ,loss:0.497 ,lr:0.16669444907484582\n",
            "epoch:5100 ,acc:0.820 ,loss:0.494 ,lr:0.16396130513198884\n",
            "epoch:5200 ,acc:0.833 ,loss:0.491 ,lr:0.16131634134537828\n",
            "epoch:5300 ,acc:0.833 ,loss:0.488 ,lr:0.15875535799333226\n",
            "epoch:5400 ,acc:0.833 ,loss:0.486 ,lr:0.1562744178777934\n",
            "epoch:5500 ,acc:0.830 ,loss:0.484 ,lr:0.15386982612709646\n",
            "epoch:5600 ,acc:0.830 ,loss:0.482 ,lr:0.15153811183512653\n",
            "epoch:5700 ,acc:0.827 ,loss:0.479 ,lr:0.14927601134497687\n",
            "epoch:5800 ,acc:0.830 ,loss:0.478 ,lr:0.14708045300779526\n",
            "epoch:5900 ,acc:0.830 ,loss:0.476 ,lr:0.14494854326714016\n",
            "epoch:6000 ,acc:0.833 ,loss:0.474 ,lr:0.1428775539362766\n",
            "epoch:6100 ,acc:0.833 ,loss:0.472 ,lr:0.1408649105507818\n",
            "epoch:6200 ,acc:0.833 ,loss:0.470 ,lr:0.13890818169190167\n",
            "epoch:6300 ,acc:0.840 ,loss:0.468 ,lr:0.13700506918755992\n",
            "epoch:6400 ,acc:0.840 ,loss:0.467 ,lr:0.13515339910798757\n",
            "epoch:6500 ,acc:0.840 ,loss:0.466 ,lr:0.13335111348179757\n",
            "epoch:6600 ,acc:0.840 ,loss:0.465 ,lr:0.13159626266614027\n",
            "epoch:6700 ,acc:0.840 ,loss:0.463 ,lr:0.12988699831146902\n",
            "epoch:6800 ,acc:0.840 ,loss:0.461 ,lr:0.12822156686754713\n",
            "epoch:6900 ,acc:0.843 ,loss:0.459 ,lr:0.126598303582732\n",
            "epoch:7000 ,acc:0.837 ,loss:0.458 ,lr:0.12501562695336915\n",
            "epoch:7100 ,acc:0.840 ,loss:0.456 ,lr:0.12347203358439313\n",
            "epoch:7200 ,acc:0.837 ,loss:0.454 ,lr:0.12196609342602757\n",
            "epoch:7300 ,acc:0.840 ,loss:0.452 ,lr:0.12049644535486204\n",
            "epoch:7400 ,acc:0.837 ,loss:0.450 ,lr:0.11906179307060363\n",
            "epoch:7500 ,acc:0.833 ,loss:0.449 ,lr:0.11766090128250381\n",
            "epoch:7600 ,acc:0.837 ,loss:0.447 ,lr:0.11629259216187929\n",
            "epoch:7700 ,acc:0.837 ,loss:0.446 ,lr:0.11495574203931487\n",
            "epoch:7800 ,acc:0.837 ,loss:0.446 ,lr:0.11364927832708263\n",
            "epoch:7900 ,acc:0.840 ,loss:0.444 ,lr:0.11237217664906168\n",
            "epoch:8000 ,acc:0.837 ,loss:0.443 ,lr:0.11112345816201799\n",
            "epoch:8100 ,acc:0.840 ,loss:0.441 ,lr:0.10990218705352237\n",
            "epoch:8200 ,acc:0.840 ,loss:0.439 ,lr:0.10870746820306555\n",
            "epoch:8300 ,acc:0.843 ,loss:0.438 ,lr:0.1075384449940854\n",
            "epoch:8400 ,acc:0.843 ,loss:0.436 ,lr:0.10639429726566654\n",
            "epoch:8500 ,acc:0.843 ,loss:0.435 ,lr:0.10527423939362038\n",
            "epoch:8600 ,acc:0.843 ,loss:0.434 ,lr:0.10417751849150952\n",
            "epoch:8700 ,acc:0.843 ,loss:0.432 ,lr:0.10310341272296113\n",
            "epoch:8800 ,acc:0.843 ,loss:0.431 ,lr:0.1020512297173181\n",
            "epoch:8900 ,acc:0.847 ,loss:0.429 ,lr:0.10102030508132134\n",
            "epoch:9000 ,acc:0.847 ,loss:0.427 ,lr:0.1000100010001\n",
            "epoch:9100 ,acc:0.847 ,loss:0.426 ,lr:0.09901970492127933\n",
            "epoch:9200 ,acc:0.847 ,loss:0.424 ,lr:0.09804882831650162\n",
            "epoch:9300 ,acc:0.843 ,loss:0.423 ,lr:0.09709680551509856\n",
            "epoch:9400 ,acc:0.847 ,loss:0.422 ,lr:0.09616309260505818\n",
            "epoch:9500 ,acc:0.847 ,loss:0.420 ,lr:0.09524716639679968\n",
            "epoch:9600 ,acc:0.850 ,loss:0.419 ,lr:0.09434852344560807\n",
            "epoch:9700 ,acc:0.853 ,loss:0.418 ,lr:0.09346667912889055\n",
            "epoch:9800 ,acc:0.853 ,loss:0.417 ,lr:0.09260116677470137\n",
            "epoch:9900 ,acc:0.853 ,loss:0.415 ,lr:0.09175153683824203\n",
            "epoch:10000 ,acc:0.850 ,loss:0.414 ,lr:0.09091735612328393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output for Momentum and Vanilla Optiimizer"
      ],
      "metadata": {
        "id": "SSqkIoldGNBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ngbJsVPRY7yd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uu84Xf49cLzq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now implementing Adam Optimizer"
      ],
      "metadata": {
        "id": "tbKfUitwGSPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adam Optimizer\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "metadata": {
        "id": "5fb7UgrGcLwa"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Layer_Dense(2, 64)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense( 64 , 3 )\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(learning_rate = 0.02, decay = 1e-5)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range ( 10001 ):\n",
        "\n",
        "    ##training\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    loss = categorical_crossentropy_loss(activation2.output, y)\n",
        "    loss_grad = categorical_crossentropy_loss_gradient(activation2.output, y) #dvalues\n",
        "\n",
        "    predictions = np.argmax(activation2.output, axis = 1 )\n",
        "    if len (y.shape) == 2 :\n",
        "      y = np.argmax(y, axis = 1 )\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    if not epoch % 100 :\n",
        "      print(f\"epoch:{epoch} ,acc:{accuracy:.3f} ,loss:{loss:.3f} ,lr:{optimizer.current_learning_rate}\")\n",
        "\n",
        "\n",
        "    # # Backward pass\n",
        "    activation2.backward(loss_grad)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Print gradients\n",
        "    # print (dense1.dweights)\n",
        "    # print (dense1.dbiases)\n",
        "    # print (dense2.dweights)\n",
        "    # print (dense2.dbiases)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()"
      ],
      "metadata": {
        "id": "1rOu1eCBcLtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9546f612-5bf5-480f-9471-a2484496b4cd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 ,acc:0.390 ,loss:1.098 ,lr:0.02\n",
            "epoch:100 ,acc:0.727 ,loss:0.715 ,lr:0.01998021958261321\n",
            "epoch:200 ,acc:0.833 ,loss:0.502 ,lr:0.019960279044701046\n",
            "epoch:300 ,acc:0.853 ,loss:0.386 ,lr:0.019940378268975763\n",
            "epoch:400 ,acc:0.900 ,loss:0.299 ,lr:0.01992051713662487\n",
            "epoch:500 ,acc:0.913 ,loss:0.256 ,lr:0.01990069552930875\n",
            "epoch:600 ,acc:0.917 ,loss:0.231 ,lr:0.019880913329158343\n",
            "epoch:700 ,acc:0.917 ,loss:0.210 ,lr:0.019861170418772778\n",
            "epoch:800 ,acc:0.923 ,loss:0.197 ,lr:0.019841466681217078\n",
            "epoch:900 ,acc:0.923 ,loss:0.187 ,lr:0.01982180200001982\n",
            "epoch:1000 ,acc:0.927 ,loss:0.179 ,lr:0.019802176259170884\n",
            "epoch:1100 ,acc:0.923 ,loss:0.172 ,lr:0.01978258934311912\n",
            "epoch:1200 ,acc:0.923 ,loss:0.165 ,lr:0.01976304113677013\n",
            "epoch:1300 ,acc:0.933 ,loss:0.157 ,lr:0.019743531525483964\n",
            "epoch:1400 ,acc:0.933 ,loss:0.152 ,lr:0.01972406039507293\n",
            "epoch:1500 ,acc:0.943 ,loss:0.145 ,lr:0.019704627631799327\n",
            "epoch:1600 ,acc:0.947 ,loss:0.137 ,lr:0.019685233122373254\n",
            "epoch:1700 ,acc:0.950 ,loss:0.132 ,lr:0.019665876753950384\n",
            "epoch:1800 ,acc:0.950 ,loss:0.127 ,lr:0.01964655841412981\n",
            "epoch:1900 ,acc:0.950 ,loss:0.123 ,lr:0.019627277990951823\n",
            "epoch:2000 ,acc:0.953 ,loss:0.120 ,lr:0.019608035372895814\n",
            "epoch:2100 ,acc:0.960 ,loss:0.118 ,lr:0.01958883044887805\n",
            "epoch:2200 ,acc:0.957 ,loss:0.114 ,lr:0.019569663108249594\n",
            "epoch:2300 ,acc:0.960 ,loss:0.112 ,lr:0.01955053324079414\n",
            "epoch:2400 ,acc:0.960 ,loss:0.110 ,lr:0.019531440736725945\n",
            "epoch:2500 ,acc:0.960 ,loss:0.108 ,lr:0.019512385486687673\n",
            "epoch:2600 ,acc:0.960 ,loss:0.107 ,lr:0.019493367381748363\n",
            "epoch:2700 ,acc:0.953 ,loss:0.109 ,lr:0.019474386313401298\n",
            "epoch:2800 ,acc:0.957 ,loss:0.103 ,lr:0.019455442173562\n",
            "epoch:2900 ,acc:0.960 ,loss:0.102 ,lr:0.019436534854566128\n",
            "epoch:3000 ,acc:0.960 ,loss:0.101 ,lr:0.01941766424916747\n",
            "epoch:3100 ,acc:0.953 ,loss:0.099 ,lr:0.019398830250535893\n",
            "epoch:3200 ,acc:0.957 ,loss:0.097 ,lr:0.019380032752255354\n",
            "epoch:3300 ,acc:0.957 ,loss:0.096 ,lr:0.01936127164832186\n",
            "epoch:3400 ,acc:0.960 ,loss:0.096 ,lr:0.01934254683314152\n",
            "epoch:3500 ,acc:0.960 ,loss:0.094 ,lr:0.019323858201528515\n",
            "epoch:3600 ,acc:0.960 ,loss:0.092 ,lr:0.019305205648703173\n",
            "epoch:3700 ,acc:0.957 ,loss:0.091 ,lr:0.01928658907028997\n",
            "epoch:3800 ,acc:0.963 ,loss:0.090 ,lr:0.01926800836231563\n",
            "epoch:3900 ,acc:0.960 ,loss:0.088 ,lr:0.019249463421207133\n",
            "epoch:4000 ,acc:0.967 ,loss:0.087 ,lr:0.019230954143789846\n",
            "epoch:4100 ,acc:0.967 ,loss:0.086 ,lr:0.019212480427285565\n",
            "epoch:4200 ,acc:0.967 ,loss:0.085 ,lr:0.019194042169310647\n",
            "epoch:4300 ,acc:0.973 ,loss:0.083 ,lr:0.019175639267874092\n",
            "epoch:4400 ,acc:0.973 ,loss:0.083 ,lr:0.019157271621375684\n",
            "epoch:4500 ,acc:0.970 ,loss:0.081 ,lr:0.0191389391286041\n",
            "epoch:4600 ,acc:0.973 ,loss:0.080 ,lr:0.019120641688735073\n",
            "epoch:4700 ,acc:0.970 ,loss:0.081 ,lr:0.019102379201329525\n",
            "epoch:4800 ,acc:0.970 ,loss:0.078 ,lr:0.01908415156633174\n",
            "epoch:4900 ,acc:0.970 ,loss:0.077 ,lr:0.01906595868406753\n",
            "epoch:5000 ,acc:0.970 ,loss:0.078 ,lr:0.01904780045524243\n",
            "epoch:5100 ,acc:0.970 ,loss:0.076 ,lr:0.019029676780939874\n",
            "epoch:5200 ,acc:0.973 ,loss:0.075 ,lr:0.019011587562619416\n",
            "epoch:5300 ,acc:0.970 ,loss:0.074 ,lr:0.01899353270211493\n",
            "epoch:5400 ,acc:0.970 ,loss:0.074 ,lr:0.018975512101632844\n",
            "epoch:5500 ,acc:0.970 ,loss:0.073 ,lr:0.018957525663750367\n",
            "epoch:5600 ,acc:0.973 ,loss:0.073 ,lr:0.018939573291413745\n",
            "epoch:5700 ,acc:0.970 ,loss:0.073 ,lr:0.018921654887936498\n",
            "epoch:5800 ,acc:0.970 ,loss:0.072 ,lr:0.018903770356997706\n",
            "epoch:5900 ,acc:0.973 ,loss:0.072 ,lr:0.018885919602640248\n",
            "epoch:6000 ,acc:0.970 ,loss:0.071 ,lr:0.018868102529269144\n",
            "epoch:6100 ,acc:0.970 ,loss:0.071 ,lr:0.018850319041649778\n",
            "epoch:6200 ,acc:0.970 ,loss:0.070 ,lr:0.018832569044906263\n",
            "epoch:6300 ,acc:0.970 ,loss:0.070 ,lr:0.018814852444519702\n",
            "epoch:6400 ,acc:0.970 ,loss:0.070 ,lr:0.018797169146326564\n",
            "epoch:6500 ,acc:0.970 ,loss:0.070 ,lr:0.01877951905651696\n",
            "epoch:6600 ,acc:0.970 ,loss:0.069 ,lr:0.018761902081633034\n",
            "epoch:6700 ,acc:0.970 ,loss:0.069 ,lr:0.018744318128567278\n",
            "epoch:6800 ,acc:0.970 ,loss:0.069 ,lr:0.018726767104560903\n",
            "epoch:6900 ,acc:0.973 ,loss:0.069 ,lr:0.018709248917202218\n",
            "epoch:7000 ,acc:0.970 ,loss:0.068 ,lr:0.018691763474424996\n",
            "epoch:7100 ,acc:0.970 ,loss:0.068 ,lr:0.018674310684506857\n",
            "epoch:7200 ,acc:0.973 ,loss:0.068 ,lr:0.01865689045606769\n",
            "epoch:7300 ,acc:0.970 ,loss:0.068 ,lr:0.01863950269806802\n",
            "epoch:7400 ,acc:0.973 ,loss:0.067 ,lr:0.018622147319807447\n",
            "epoch:7500 ,acc:0.973 ,loss:0.068 ,lr:0.018604824230923075\n",
            "epoch:7600 ,acc:0.973 ,loss:0.067 ,lr:0.01858753334138793\n",
            "epoch:7700 ,acc:0.973 ,loss:0.067 ,lr:0.018570274561509396\n",
            "epoch:7800 ,acc:0.973 ,loss:0.067 ,lr:0.018553047801927663\n",
            "epoch:7900 ,acc:0.973 ,loss:0.066 ,lr:0.018535852973614212\n",
            "epoch:8000 ,acc:0.973 ,loss:0.066 ,lr:0.01851868998787026\n",
            "epoch:8100 ,acc:0.973 ,loss:0.066 ,lr:0.018501558756325222\n",
            "epoch:8200 ,acc:0.973 ,loss:0.066 ,lr:0.01848445919093522\n",
            "epoch:8300 ,acc:0.973 ,loss:0.066 ,lr:0.018467391203981567\n",
            "epoch:8400 ,acc:0.973 ,loss:0.067 ,lr:0.018450354708069265\n",
            "epoch:8500 ,acc:0.973 ,loss:0.065 ,lr:0.018433349616125496\n",
            "epoch:8600 ,acc:0.973 ,loss:0.065 ,lr:0.018416375841398172\n",
            "epoch:8700 ,acc:0.973 ,loss:0.065 ,lr:0.01839943329745444\n",
            "epoch:8800 ,acc:0.973 ,loss:0.065 ,lr:0.01838252189817921\n",
            "epoch:8900 ,acc:0.973 ,loss:0.065 ,lr:0.018365641557773718\n",
            "epoch:9000 ,acc:0.973 ,loss:0.065 ,lr:0.018348792190754044\n",
            "epoch:9100 ,acc:0.973 ,loss:0.065 ,lr:0.0183319737119497\n",
            "epoch:9200 ,acc:0.973 ,loss:0.065 ,lr:0.018315186036502167\n",
            "epoch:9300 ,acc:0.973 ,loss:0.064 ,lr:0.018298429079863496\n",
            "epoch:9400 ,acc:0.973 ,loss:0.064 ,lr:0.018281702757794862\n",
            "epoch:9500 ,acc:0.977 ,loss:0.064 ,lr:0.018265006986365174\n",
            "epoch:9600 ,acc:0.973 ,loss:0.064 ,lr:0.018248341681949654\n",
            "epoch:9700 ,acc:0.973 ,loss:0.064 ,lr:0.018231706761228456\n",
            "epoch:9800 ,acc:0.973 ,loss:0.064 ,lr:0.018215102141185255\n",
            "epoch:9900 ,acc:0.977 ,loss:0.064 ,lr:0.018198527739105907\n",
            "epoch:10000 ,acc:0.973 ,loss:0.065 ,lr:0.018181983472577025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output with Adam Optimizer"
      ],
      "metadata": {
        "id": "3SBkbwcIGjVf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ss0cZL6ecLqy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem of overfitting may arrive, So adding regularization terms to dense layers to avoid overfitting"
      ],
      "metadata": {
        "id": "l9sWYIIIGrMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons,\n",
        "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "        # Set regularization strength\n",
        "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradients on regularization\n",
        "        # L1 on weights\n",
        "        if self.weight_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.weights)\n",
        "            dL1[self.weights < 0] = -1\n",
        "            self.dweights += self.weight_regularizer_l1 * dL1\n",
        "        # L2 on weights\n",
        "        if self.weight_regularizer_l2 > 0:\n",
        "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
        "                             self.weights\n",
        "        # L1 on biases\n",
        "        if self.bias_regularizer_l1 > 0:\n",
        "            dL1 = np.ones_like(self.biases)\n",
        "            dL1[self.biases < 0] = -1\n",
        "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "        # L2 on biases\n",
        "        if self.bias_regularizer_l2 > 0:\n",
        "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
        "                            self.biases\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
      ],
      "metadata": {
        "id": "Fg6NN4vOGd47"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMWWGtnK9Wd5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwLE5TOT9WM7"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIEXrKXS9WJW"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization loss calculate function"
      ],
      "metadata": {
        "id": "YS10X81JHDgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization loss calculation\n",
        "def regularization_loss(layer):\n",
        "    # 0 by default\n",
        "    regularization_loss = 0\n",
        "    # L1 regularization - weights\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.weight_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l1 * \\\n",
        "      np.sum(np.abs(layer.weights))\n",
        "    # L2 regularization - weights\n",
        "    if layer.weight_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.weight_regularizer_l2 * \\\n",
        "      np.sum(layer.weights * \\\n",
        "      layer.weights)\n",
        "\n",
        "    # L1 regularization - biases\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.bias_regularizer_l1 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l1 * \\\n",
        "      np.sum(np.abs(layer.biases))\n",
        "    # L2 regularization - biases\n",
        "    if layer.bias_regularizer_l2 > 0 :\n",
        "      regularization_loss += layer.bias_regularizer_l2 * \\\n",
        "      np.sum(layer.biases * \\\n",
        "      layer.biases)\n",
        "    return regularization_loss"
      ],
      "metadata": {
        "id": "1xx5Jd385CZZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with regularization"
      ],
      "metadata": {
        "id": "gRrrbFy1HSwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = spiral_data( samples = 100 , classes = 3 )\n",
        "\n",
        "dense1 = Layer_Dense(2, 64, weight_regularizer_l2 = 5e-4, bias_regularizer_l2 = 5e-4)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense( 64 , 3 )\n",
        "activation2 = Activation_Softmax()\n",
        "# loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(learning_rate = 0.02 , decay = 5e-7)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range ( 10001 ):\n",
        "\n",
        "    ##training\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    activation2.forward(dense2.output)\n",
        "\n",
        "    data_loss = categorical_crossentropy_loss(activation2.output, y)\n",
        "    loss_grad = categorical_crossentropy_loss_gradient(activation2.output, y) #dvalues\n",
        "\n",
        "    regular_loss = regularization_loss(dense1) + regularization_loss(dense2)\n",
        "    # loss = loss_activation.forward(dense2.output, y)\n",
        "    loss = data_loss + regular_loss\n",
        "    predictions = np.argmax(activation2.output, axis = 1 )\n",
        "    if len (y.shape) == 2 :\n",
        "      y = np.argmax(y, axis = 1 )\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    if not epoch % 100 :\n",
        "      print(f\"epoch:{epoch} ,acc:{accuracy:.3f} ,loss:{loss:.3f} , regular_loss: {regular_loss}, lr:{optimizer.current_learning_rate}\")\n",
        "\n",
        "\n",
        "    # # Backward pass\n",
        "    activation2.backward(loss_grad)\n",
        "    dense2.backward(activation2.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Print gradients\n",
        "    # print (dense1.dweights)\n",
        "    # print (dense1.dbiases)\n",
        "    # print (dense2.dweights)\n",
        "    # print (dense2.dbiases)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()"
      ],
      "metadata": {
        "id": "YAsEttUexJjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dacf6cc-26ff-41d6-e233-97ba8c681a85"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 ,acc:0.277 ,loss:1.099 , regular_loss: 6.427696789317836e-06, lr:0.02\n",
            "epoch:100 ,acc:0.587 ,loss:0.921 , regular_loss: 0.02475313947586476, lr:0.019999010049002574\n",
            "epoch:200 ,acc:0.720 ,loss:0.756 , regular_loss: 0.05934568290388974, lr:0.019998010197985302\n",
            "epoch:300 ,acc:0.790 ,loss:0.667 , regular_loss: 0.08226228737560022, lr:0.019997010446938183\n",
            "epoch:400 ,acc:0.823 ,loss:0.624 , regular_loss: 0.09371363482855283, lr:0.01999601079584623\n",
            "epoch:500 ,acc:0.840 ,loss:0.584 , regular_loss: 0.0999552241905841, lr:0.01999501124469445\n",
            "epoch:600 ,acc:0.850 ,loss:0.557 , regular_loss: 0.103155734476285, lr:0.01999401179346786\n",
            "epoch:700 ,acc:0.857 ,loss:0.536 , regular_loss: 0.1046686584102526, lr:0.01999301244215147\n",
            "epoch:800 ,acc:0.870 ,loss:0.519 , regular_loss: 0.10527175925671167, lr:0.0199920131907303\n",
            "epoch:900 ,acc:0.863 ,loss:0.504 , regular_loss: 0.10542987234675266, lr:0.019991014039189386\n",
            "epoch:1000 ,acc:0.870 ,loss:0.491 , regular_loss: 0.10504053664500443, lr:0.019990014987513734\n",
            "epoch:1100 ,acc:0.883 ,loss:0.475 , regular_loss: 0.10445032184935102, lr:0.01998901603568839\n",
            "epoch:1200 ,acc:0.890 ,loss:0.463 , regular_loss: 0.10351343410083486, lr:0.019988017183698373\n",
            "epoch:1300 ,acc:0.860 ,loss:0.454 , regular_loss: 0.1024114431312936, lr:0.01998701843152872\n",
            "epoch:1400 ,acc:0.873 ,loss:0.444 , regular_loss: 0.10126203713923992, lr:0.019986019779164473\n",
            "epoch:1500 ,acc:0.893 ,loss:0.436 , regular_loss: 0.10016461198985702, lr:0.019985021226590672\n",
            "epoch:1600 ,acc:0.890 ,loss:0.427 , regular_loss: 0.09941082508421446, lr:0.01998402277379235\n",
            "epoch:1700 ,acc:0.873 ,loss:0.424 , regular_loss: 0.09843672730181745, lr:0.01998302442075457\n",
            "epoch:1800 ,acc:0.883 ,loss:0.415 , regular_loss: 0.09733395386670181, lr:0.019982026167462367\n",
            "epoch:1900 ,acc:0.893 ,loss:0.410 , regular_loss: 0.09636418821281409, lr:0.019981028013900805\n",
            "epoch:2000 ,acc:0.880 ,loss:0.405 , regular_loss: 0.09529279655251197, lr:0.019980029960054924\n",
            "epoch:2100 ,acc:0.893 ,loss:0.409 , regular_loss: 0.09432179379334943, lr:0.019979032005909798\n",
            "epoch:2200 ,acc:0.897 ,loss:0.396 , regular_loss: 0.09337680312788331, lr:0.01997803415145048\n",
            "epoch:2300 ,acc:0.893 ,loss:0.390 , regular_loss: 0.09246416862357212, lr:0.019977036396662037\n",
            "epoch:2400 ,acc:0.893 ,loss:0.386 , regular_loss: 0.09154638486012928, lr:0.019976038741529537\n",
            "epoch:2500 ,acc:0.900 ,loss:0.382 , regular_loss: 0.09073048215901934, lr:0.01997504118603805\n",
            "epoch:2600 ,acc:0.900 ,loss:0.378 , regular_loss: 0.08994602329739139, lr:0.01997404373017264\n",
            "epoch:2700 ,acc:0.910 ,loss:0.377 , regular_loss: 0.08913729609031844, lr:0.0199730463739184\n",
            "epoch:2800 ,acc:0.897 ,loss:0.370 , regular_loss: 0.08819653897256001, lr:0.019972049117260395\n",
            "epoch:2900 ,acc:0.897 ,loss:0.368 , regular_loss: 0.08713593710668215, lr:0.019971051960183714\n",
            "epoch:3000 ,acc:0.903 ,loss:0.362 , regular_loss: 0.08613337419160767, lr:0.019970054902673444\n",
            "epoch:3100 ,acc:0.913 ,loss:0.358 , regular_loss: 0.08519454106185834, lr:0.019969057944714663\n",
            "epoch:3200 ,acc:0.900 ,loss:0.356 , regular_loss: 0.08428518978804482, lr:0.019968061086292475\n",
            "epoch:3300 ,acc:0.910 ,loss:0.353 , regular_loss: 0.08344332921535821, lr:0.019967064327391967\n",
            "epoch:3400 ,acc:0.910 ,loss:0.352 , regular_loss: 0.08264938861391827, lr:0.019966067667998237\n",
            "epoch:3500 ,acc:0.907 ,loss:0.353 , regular_loss: 0.08187380389155831, lr:0.019965071108096383\n",
            "epoch:3600 ,acc:0.910 ,loss:0.345 , regular_loss: 0.08117175622156311, lr:0.01996407464767152\n",
            "epoch:3700 ,acc:0.903 ,loss:0.342 , regular_loss: 0.08046615753204547, lr:0.019963078286708732\n",
            "epoch:3800 ,acc:0.910 ,loss:0.344 , regular_loss: 0.07973141839196862, lr:0.019962082025193145\n",
            "epoch:3900 ,acc:0.910 ,loss:0.339 , regular_loss: 0.0790967475671237, lr:0.019961085863109868\n",
            "epoch:4000 ,acc:0.917 ,loss:0.335 , regular_loss: 0.0784368624197111, lr:0.019960089800444013\n",
            "epoch:4100 ,acc:0.910 ,loss:0.337 , regular_loss: 0.07778358002342868, lr:0.019959093837180697\n",
            "epoch:4200 ,acc:0.907 ,loss:0.334 , regular_loss: 0.07718960872553474, lr:0.01995809797330505\n",
            "epoch:4300 ,acc:0.903 ,loss:0.330 , regular_loss: 0.0765933380639092, lr:0.01995710220880218\n",
            "epoch:4400 ,acc:0.900 ,loss:0.336 , regular_loss: 0.07604643503748931, lr:0.019956106543657228\n",
            "epoch:4500 ,acc:0.897 ,loss:0.347 , regular_loss: 0.07547205374955018, lr:0.019955110977855316\n",
            "epoch:4600 ,acc:0.907 ,loss:0.330 , regular_loss: 0.07497847695181434, lr:0.01995411551138158\n",
            "epoch:4700 ,acc:0.917 ,loss:0.322 , regular_loss: 0.07442322630749908, lr:0.019953120144221154\n",
            "epoch:4800 ,acc:0.913 ,loss:0.322 , regular_loss: 0.07388103657778715, lr:0.019952124876359174\n",
            "epoch:4900 ,acc:0.913 ,loss:0.319 , regular_loss: 0.07345676092552489, lr:0.01995112970778079\n",
            "epoch:5000 ,acc:0.913 ,loss:0.315 , regular_loss: 0.07291068407304363, lr:0.019950134638471142\n",
            "epoch:5100 ,acc:0.917 ,loss:0.316 , regular_loss: 0.07244548762743303, lr:0.019949139668415376\n",
            "epoch:5200 ,acc:0.907 ,loss:0.314 , regular_loss: 0.07191692238757642, lr:0.01994814479759864\n",
            "epoch:5300 ,acc:0.913 ,loss:0.312 , regular_loss: 0.07144984987133911, lr:0.019947150026006097\n",
            "epoch:5400 ,acc:0.913 ,loss:0.310 , regular_loss: 0.07099742425269213, lr:0.019946155353622895\n",
            "epoch:5500 ,acc:0.923 ,loss:0.309 , regular_loss: 0.07053570272746386, lr:0.019945160780434196\n",
            "epoch:5600 ,acc:0.913 ,loss:0.313 , regular_loss: 0.07013847560382477, lr:0.019944166306425162\n",
            "epoch:5700 ,acc:0.910 ,loss:0.309 , regular_loss: 0.0697193859060653, lr:0.01994317193158096\n",
            "epoch:5800 ,acc:0.917 ,loss:0.307 , regular_loss: 0.0692995079444612, lr:0.019942177655886757\n",
            "epoch:5900 ,acc:0.910 ,loss:0.304 , regular_loss: 0.06887204428132282, lr:0.019941183479327725\n",
            "epoch:6000 ,acc:0.917 ,loss:0.302 , regular_loss: 0.06850277124247853, lr:0.019940189401889033\n",
            "epoch:6100 ,acc:0.913 ,loss:0.301 , regular_loss: 0.06810561783471192, lr:0.01993919542355587\n",
            "epoch:6200 ,acc:0.907 ,loss:0.300 , regular_loss: 0.06770669052247462, lr:0.019938201544313403\n",
            "epoch:6300 ,acc:0.913 ,loss:0.313 , regular_loss: 0.06735082762837856, lr:0.01993720776414682\n",
            "epoch:6400 ,acc:0.913 ,loss:0.296 , regular_loss: 0.06695079988453212, lr:0.019936214083041307\n",
            "epoch:6500 ,acc:0.913 ,loss:0.296 , regular_loss: 0.06659439870945053, lr:0.01993522050098206\n",
            "epoch:6600 ,acc:0.903 ,loss:0.298 , regular_loss: 0.06625657594789316, lr:0.019934227017954262\n",
            "epoch:6700 ,acc:0.907 ,loss:0.298 , regular_loss: 0.06593241420682357, lr:0.01993323363394311\n",
            "epoch:6800 ,acc:0.917 ,loss:0.298 , regular_loss: 0.06559315417942885, lr:0.0199322403489338\n",
            "epoch:6900 ,acc:0.913 ,loss:0.291 , regular_loss: 0.06520254641468708, lr:0.019931247162911534\n",
            "epoch:7000 ,acc:0.917 ,loss:0.290 , regular_loss: 0.06488212803561488, lr:0.019930254075861523\n",
            "epoch:7100 ,acc:0.927 ,loss:0.291 , regular_loss: 0.06458792614607743, lr:0.019929261087768962\n",
            "epoch:7200 ,acc:0.917 ,loss:0.288 , regular_loss: 0.06425264178905739, lr:0.01992826819861907\n",
            "epoch:7300 ,acc:0.917 ,loss:0.287 , regular_loss: 0.06391542322451062, lr:0.019927275408397054\n",
            "epoch:7400 ,acc:0.917 ,loss:0.294 , regular_loss: 0.06363590586887616, lr:0.019926282717088132\n",
            "epoch:7500 ,acc:0.917 ,loss:0.286 , regular_loss: 0.06331591680479493, lr:0.01992529012467752\n",
            "epoch:7600 ,acc:0.910 ,loss:0.288 , regular_loss: 0.0630285800716279, lr:0.019924297631150445\n",
            "epoch:7700 ,acc:0.913 ,loss:0.286 , regular_loss: 0.06274620032234242, lr:0.019923305236492123\n",
            "epoch:7800 ,acc:0.927 ,loss:0.283 , regular_loss: 0.06246090306718555, lr:0.01992231294068779\n",
            "epoch:7900 ,acc:0.913 ,loss:0.282 , regular_loss: 0.0621630343611333, lr:0.019921320743722666\n",
            "epoch:8000 ,acc:0.913 ,loss:0.281 , regular_loss: 0.061869405252716086, lr:0.019920328645582\n",
            "epoch:8100 ,acc:0.910 ,loss:0.280 , regular_loss: 0.061584114382697105, lr:0.019919336646251007\n",
            "epoch:8200 ,acc:0.907 ,loss:0.280 , regular_loss: 0.06134507681258239, lr:0.019918344745714942\n",
            "epoch:8300 ,acc:0.870 ,loss:0.473 , regular_loss: 0.06620553488680776, lr:0.019917352943959042\n",
            "epoch:8400 ,acc:0.903 ,loss:0.393 , regular_loss: 0.06588173025665262, lr:0.019916361240968555\n",
            "epoch:8500 ,acc:0.907 ,loss:0.388 , regular_loss: 0.0653148400399576, lr:0.01991536963672872\n",
            "epoch:8600 ,acc:0.907 ,loss:0.387 , regular_loss: 0.0648299380065641, lr:0.019914378131224802\n",
            "epoch:8700 ,acc:0.907 ,loss:0.385 , regular_loss: 0.06436255950246066, lr:0.01991338672444204\n",
            "epoch:8800 ,acc:0.900 ,loss:0.384 , regular_loss: 0.06392063072286544, lr:0.0199123954163657\n",
            "epoch:8900 ,acc:0.900 ,loss:0.383 , regular_loss: 0.06349722157205515, lr:0.019911404206981037\n",
            "epoch:9000 ,acc:0.907 ,loss:0.382 , regular_loss: 0.0630865030803018, lr:0.019910413096273318\n",
            "epoch:9100 ,acc:0.907 ,loss:0.381 , regular_loss: 0.06269254187886525, lr:0.019909422084227805\n",
            "epoch:9200 ,acc:0.907 ,loss:0.381 , regular_loss: 0.06231263014952085, lr:0.019908431170829768\n",
            "epoch:9300 ,acc:0.903 ,loss:0.380 , regular_loss: 0.06193859406116706, lr:0.01990744035606448\n",
            "epoch:9400 ,acc:0.907 ,loss:0.379 , regular_loss: 0.06157326956587752, lr:0.01990644963991721\n",
            "epoch:9500 ,acc:0.903 ,loss:0.379 , regular_loss: 0.06122024128134581, lr:0.01990545902237324\n",
            "epoch:9600 ,acc:0.917 ,loss:0.382 , regular_loss: 0.060884016682477726, lr:0.019904468503417844\n",
            "epoch:9700 ,acc:0.903 ,loss:0.378 , regular_loss: 0.060603975799182655, lr:0.019903478083036316\n",
            "epoch:9800 ,acc:0.913 ,loss:0.382 , regular_loss: 0.0603354577074394, lr:0.019902487761213932\n",
            "epoch:9900 ,acc:0.900 ,loss:0.378 , regular_loss: 0.06006958985383918, lr:0.019901497537935988\n",
            "epoch:10000 ,acc:0.917 ,loss:0.378 , regular_loss: 0.05979999530087017, lr:0.019900507413187767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally tested all our different layers"
      ],
      "metadata": {
        "id": "cvGtwGQlHWsh"
      }
    }
  ]
}